{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "from dnc import DNC\n",
    "import torchvision as tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tv.datasets.MNIST('.', train=True, transform=tv.transforms.ToTensor())\n",
    "test = tv.datasets.MNIST('.', transform=tv.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = T.utils.data.DataLoader(dataset=train, batch_size=50, shuffle=True)\n",
    "trainset = iter(train_data_loader)\n",
    "trainsize = len(train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffy = DNC(28, 128, num_layers=5, independent_linears=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(controller_hidden, memory, read_vectors) = (None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('DNC-model-2', 'rb')\n",
    "diffy = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('DNC-memory-2', 'rb')\n",
    "memory = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = T.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changed the learning rate from 0.01 to 0.0001 after 2 epochs then the 3rd gets 0.00001\n",
    "\n",
    "It appears I have to reduce the learning rate once it gets into the ballpark or else it just messes up\n",
    "\n",
    "0.01 to 0.001 to 0.0001 lr\n",
    "\n",
    "30 epochs so far - accuracy 99.62333333333333%\n",
    "\n",
    "Currently testing 31st ep\n",
    "\n",
    "Train time is 1 hr\n",
    "\n",
    "Test time is 20 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = T.optim.Adam(diffy.parameters(), lr=0.00001, eps=1e-9, betas=[0.9, 0.98])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = trainsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1, Loss: 0.0015460527502000332\n",
      "Step: 2, Loss: 0.002151911146938801\n",
      "Step: 3, Loss: 0.004132685251533985\n",
      "Step: 4, Loss: 0.0007312879897654057\n",
      "Step: 5, Loss: 0.06569927930831909\n",
      "Step: 6, Loss: 0.002843863796442747\n",
      "Step: 7, Loss: 0.003088747849687934\n",
      "Step: 8, Loss: 0.0038875455502420664\n",
      "Step: 9, Loss: 0.0042493282817304134\n",
      "Step: 10, Loss: 0.0033784324768930674\n",
      "Step: 11, Loss: 0.0020524668507277966\n",
      "Step: 12, Loss: 0.002213516039773822\n",
      "Step: 13, Loss: 0.0017683504847809672\n",
      "Step: 14, Loss: 0.004150979686528444\n",
      "Step: 15, Loss: 0.0017921667313203216\n",
      "Step: 16, Loss: 0.0013758089626207948\n",
      "Step: 17, Loss: 0.012639118358492851\n",
      "Step: 18, Loss: 0.0021611552219837904\n",
      "Step: 19, Loss: 0.00189704739023\n",
      "Step: 20, Loss: 0.0020149946212768555\n",
      "Step: 21, Loss: 0.0026519859675318003\n",
      "Step: 22, Loss: 0.0017503317212685943\n",
      "Step: 23, Loss: 0.003079513553529978\n",
      "Step: 24, Loss: 0.0037219126243144274\n",
      "Step: 25, Loss: 0.0037502581253647804\n",
      "Step: 26, Loss: 0.0031490174587816\n",
      "Step: 27, Loss: 0.002261032350361347\n",
      "Step: 28, Loss: 0.0021227672696113586\n",
      "Step: 29, Loss: 0.002842298708856106\n",
      "Step: 30, Loss: 0.002935257740318775\n",
      "Step: 31, Loss: 0.0038802092894911766\n",
      "Step: 32, Loss: 0.0017611525254324079\n",
      "Step: 33, Loss: 0.0036027259193360806\n",
      "Step: 34, Loss: 0.005302927456796169\n",
      "Step: 35, Loss: 0.002023194683715701\n",
      "Step: 36, Loss: 0.0017961396370083094\n",
      "Step: 37, Loss: 0.0022677434608340263\n",
      "Step: 38, Loss: 0.09251664578914642\n",
      "Step: 39, Loss: 0.008882194757461548\n",
      "Step: 40, Loss: 0.00491902744397521\n",
      "Step: 41, Loss: 0.003406867617741227\n",
      "Step: 42, Loss: 0.002495218301191926\n",
      "Step: 43, Loss: 0.006061059422791004\n",
      "Step: 44, Loss: 0.0019199508242309093\n",
      "Step: 45, Loss: 0.018535282462835312\n",
      "Step: 46, Loss: 0.0024170279502868652\n",
      "Step: 47, Loss: 0.00459465803578496\n",
      "Step: 48, Loss: 0.002235134830698371\n",
      "Step: 49, Loss: 0.06854955106973648\n",
      "Step: 50, Loss: 0.001622237847186625\n",
      "Step: 51, Loss: 0.0020707075018435717\n",
      "Step: 52, Loss: 0.006306543946266174\n",
      "Step: 53, Loss: 0.005954212509095669\n",
      "Step: 54, Loss: 0.0034770825877785683\n",
      "Step: 55, Loss: 0.0017044016858562827\n",
      "Step: 56, Loss: 0.00819468218833208\n",
      "Step: 57, Loss: 0.0025864806957542896\n",
      "Step: 58, Loss: 0.002950295340269804\n",
      "Step: 59, Loss: 0.02010943554341793\n",
      "Step: 60, Loss: 0.29447314143180847\n",
      "Step: 61, Loss: 0.0028263984713703394\n",
      "Step: 62, Loss: 0.0014007074059918523\n",
      "Step: 63, Loss: 0.0014481772668659687\n",
      "Step: 64, Loss: 0.0022735788952559233\n",
      "Step: 65, Loss: 0.0015295757912099361\n",
      "Step: 66, Loss: 0.0036086239852011204\n",
      "Step: 67, Loss: 0.0017497147200629115\n",
      "Step: 68, Loss: 0.0016915866872295737\n",
      "Step: 69, Loss: 0.015605912543833256\n",
      "Step: 70, Loss: 0.0030406576115638018\n",
      "Step: 71, Loss: 0.005754572805017233\n",
      "Step: 72, Loss: 0.0015907835913822055\n",
      "Step: 73, Loss: 0.7125564813613892\n",
      "Step: 74, Loss: 0.02236943319439888\n",
      "Step: 75, Loss: 0.010277098044753075\n",
      "Step: 76, Loss: 0.0023436755873262882\n",
      "Step: 77, Loss: 0.0012014901731163263\n",
      "Step: 78, Loss: 0.003664908930659294\n",
      "Step: 79, Loss: 0.05166858807206154\n",
      "Step: 80, Loss: 0.0030495007522404194\n",
      "Step: 81, Loss: 0.0017517766682431102\n",
      "Step: 82, Loss: 0.00329605583101511\n",
      "Step: 83, Loss: 0.0014916514046490192\n",
      "Step: 84, Loss: 0.0030692103318870068\n",
      "Step: 85, Loss: 0.001858909148722887\n",
      "Step: 86, Loss: 0.007198333274573088\n",
      "Step: 87, Loss: 0.002325993264093995\n",
      "Step: 88, Loss: 0.002707140753045678\n",
      "Step: 89, Loss: 0.0029765679500997066\n",
      "Step: 90, Loss: 0.003112382721155882\n",
      "Step: 91, Loss: 0.0018172846175730228\n",
      "Step: 92, Loss: 0.002644937951117754\n",
      "Step: 93, Loss: 0.00445680832490325\n",
      "Step: 94, Loss: 0.006452466361224651\n",
      "Step: 95, Loss: 0.003134181722998619\n",
      "Step: 96, Loss: 0.001634401036426425\n",
      "Step: 97, Loss: 0.002861645305529237\n",
      "Step: 98, Loss: 0.002691290806978941\n",
      "Step: 99, Loss: 0.0013974567409604788\n",
      "Step: 100, Loss: 0.05104724317789078\n",
      "Step: 101, Loss: 0.0021957613062113523\n",
      "Step: 102, Loss: 0.00853041186928749\n",
      "Step: 103, Loss: 0.006499723065644503\n",
      "Step: 104, Loss: 0.005161838140338659\n",
      "Step: 105, Loss: 0.0021500305738300085\n",
      "Step: 106, Loss: 0.06736686825752258\n",
      "Step: 107, Loss: 0.001161949010565877\n",
      "Step: 108, Loss: 0.0020878624636679888\n",
      "Step: 109, Loss: 0.004993613809347153\n",
      "Step: 110, Loss: 0.0038699693977832794\n",
      "Step: 111, Loss: 0.0027264636009931564\n",
      "Step: 112, Loss: 0.0021036099642515182\n",
      "Step: 113, Loss: 0.002116585150361061\n",
      "Step: 114, Loss: 0.003938761074095964\n",
      "Step: 115, Loss: 0.0018861963180825114\n",
      "Step: 116, Loss: 0.00555774150416255\n",
      "Step: 117, Loss: 0.0022637012880295515\n",
      "Step: 118, Loss: 0.001530802110210061\n",
      "Step: 119, Loss: 0.00540999136865139\n",
      "Step: 120, Loss: 0.014315241016447544\n",
      "Step: 121, Loss: 0.0031659333035349846\n",
      "Step: 122, Loss: 0.016054166480898857\n",
      "Step: 123, Loss: 0.0025790927466005087\n",
      "Step: 124, Loss: 0.0021021049469709396\n",
      "Step: 125, Loss: 0.004449514206498861\n",
      "Step: 126, Loss: 0.0026765286456793547\n",
      "Step: 127, Loss: 0.00198552617803216\n",
      "Step: 128, Loss: 0.0021001386921852827\n",
      "Step: 129, Loss: 0.001538325333967805\n",
      "Step: 130, Loss: 0.001540611032396555\n",
      "Step: 131, Loss: 0.0013255106750875711\n",
      "Step: 132, Loss: 0.0018246013205498457\n",
      "Step: 133, Loss: 0.002132783178240061\n",
      "Step: 134, Loss: 0.004553292412310839\n",
      "Step: 135, Loss: 0.0017339892219752073\n",
      "Step: 136, Loss: 0.0013449800899252295\n",
      "Step: 137, Loss: 0.0031528440304100513\n",
      "Step: 138, Loss: 0.019202450290322304\n",
      "Step: 139, Loss: 0.00468280166387558\n",
      "Step: 140, Loss: 0.001740975072607398\n",
      "Step: 141, Loss: 0.0028235255740582943\n",
      "Step: 142, Loss: 0.019791530445218086\n",
      "Step: 143, Loss: 0.0019417180446907878\n",
      "Step: 144, Loss: 0.0023421389050781727\n",
      "Step: 145, Loss: 0.001872396213002503\n",
      "Step: 146, Loss: 0.00198696949519217\n",
      "Step: 147, Loss: 0.005709122400730848\n",
      "Step: 148, Loss: 0.0020725505892187357\n",
      "Step: 149, Loss: 0.0017221826128661633\n",
      "Step: 150, Loss: 0.05413708835840225\n",
      "Step: 151, Loss: 0.001568712992593646\n",
      "Step: 152, Loss: 0.0017958249663934112\n",
      "Step: 153, Loss: 0.002472344320267439\n",
      "Step: 154, Loss: 0.0030845794826745987\n",
      "Step: 155, Loss: 0.0018001777352765203\n",
      "Step: 156, Loss: 0.003812589682638645\n",
      "Step: 157, Loss: 0.006314937025308609\n",
      "Step: 158, Loss: 0.0029765975195914507\n",
      "Step: 159, Loss: 0.027818473055958748\n",
      "Step: 160, Loss: 0.002359742997214198\n",
      "Step: 161, Loss: 0.13062599301338196\n",
      "Step: 162, Loss: 0.0015441980212926865\n",
      "Step: 163, Loss: 0.0072122556157410145\n",
      "Step: 164, Loss: 0.0011007972061634064\n",
      "Step: 165, Loss: 0.0015861238352954388\n",
      "Step: 166, Loss: 0.004894008859992027\n",
      "Step: 167, Loss: 0.0031558272894471884\n",
      "Step: 168, Loss: 0.001516841002739966\n",
      "Step: 169, Loss: 0.0011425454868003726\n",
      "Step: 170, Loss: 0.002264880808070302\n",
      "Step: 171, Loss: 0.0032021484803408384\n",
      "Step: 172, Loss: 0.0014259699964895844\n",
      "Step: 173, Loss: 0.0026143891736865044\n",
      "Step: 174, Loss: 0.016370847821235657\n",
      "Step: 175, Loss: 0.0015060618752613664\n",
      "Step: 176, Loss: 0.0056826286017894745\n",
      "Step: 177, Loss: 0.0016020135954022408\n",
      "Step: 178, Loss: 0.0011621047742664814\n",
      "Step: 179, Loss: 0.001585056190378964\n",
      "Step: 180, Loss: 0.0023731382098048925\n",
      "Step: 181, Loss: 0.0026149677578359842\n",
      "Step: 182, Loss: 0.002880284795537591\n",
      "Step: 183, Loss: 0.010652836412191391\n",
      "Step: 184, Loss: 0.0016015419969335198\n",
      "Step: 185, Loss: 0.003208814188838005\n",
      "Step: 186, Loss: 0.0075004068203270435\n",
      "Step: 187, Loss: 0.002180024515837431\n",
      "Step: 188, Loss: 0.0054938639514148235\n",
      "Step: 189, Loss: 0.001573646441102028\n",
      "Step: 190, Loss: 0.0028867388609796762\n",
      "Step: 191, Loss: 0.08160155266523361\n",
      "Step: 192, Loss: 0.0017641923623159528\n",
      "Step: 193, Loss: 0.002181831281632185\n",
      "Step: 194, Loss: 0.0036310816649347544\n",
      "Step: 195, Loss: 0.004387042485177517\n",
      "Step: 196, Loss: 0.0014272172702476382\n",
      "Step: 197, Loss: 0.002570823533460498\n",
      "Step: 198, Loss: 0.002167372964322567\n",
      "Step: 199, Loss: 0.013469121418893337\n",
      "Step: 200, Loss: 0.0047692167572677135\n",
      "Step: 201, Loss: 0.00843121949583292\n",
      "Step: 202, Loss: 0.0019149950239807367\n",
      "Step: 203, Loss: 0.0027809948660433292\n",
      "Step: 204, Loss: 0.003012344939634204\n",
      "Step: 205, Loss: 0.07785863429307938\n",
      "Step: 206, Loss: 0.0020707487128674984\n",
      "Step: 207, Loss: 0.0100110387429595\n",
      "Step: 208, Loss: 0.005053076893091202\n",
      "Step: 209, Loss: 0.013559216633439064\n",
      "Step: 210, Loss: 0.001213221112266183\n",
      "Step: 211, Loss: 0.003334705950692296\n",
      "Step: 212, Loss: 0.004752900917083025\n",
      "Step: 213, Loss: 0.0038713309913873672\n",
      "Step: 214, Loss: 0.0052164769731462\n",
      "Step: 215, Loss: 0.004378678277134895\n",
      "Step: 216, Loss: 0.003887106664478779\n",
      "Step: 217, Loss: 0.0037152289878576994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 218, Loss: 0.005732440389692783\n",
      "Step: 219, Loss: 0.0011277416488155723\n",
      "Step: 220, Loss: 0.002915211720392108\n",
      "Step: 221, Loss: 0.08179882913827896\n",
      "Step: 222, Loss: 0.004470489453524351\n",
      "Step: 223, Loss: 0.014082363806664944\n",
      "Step: 224, Loss: 0.0023116276133805513\n",
      "Step: 225, Loss: 0.002104909857735038\n",
      "Step: 226, Loss: 0.005461986642330885\n",
      "Step: 227, Loss: 0.00206398987211287\n",
      "Step: 228, Loss: 0.0023501955438405275\n",
      "Step: 229, Loss: 0.4892539083957672\n",
      "Step: 230, Loss: 0.0015284972032532096\n",
      "Step: 231, Loss: 0.0024673736188560724\n",
      "Step: 232, Loss: 0.0019567282870411873\n",
      "Step: 233, Loss: 0.004494038876146078\n",
      "Step: 234, Loss: 0.0026634896639734507\n",
      "Step: 235, Loss: 0.0026085362769663334\n",
      "Step: 236, Loss: 0.0022002689074724913\n",
      "Step: 237, Loss: 0.002536618849262595\n",
      "Step: 238, Loss: 0.0017253031255677342\n",
      "Step: 239, Loss: 0.004119072575122118\n",
      "Step: 240, Loss: 0.0010770120425149798\n",
      "Step: 241, Loss: 0.010989579372107983\n",
      "Step: 242, Loss: 0.0017148187616840005\n",
      "Step: 243, Loss: 0.0017031446332111955\n",
      "Step: 244, Loss: 0.004100287798792124\n",
      "Step: 245, Loss: 0.0028919177129864693\n",
      "Step: 246, Loss: 0.0020563285797834396\n",
      "Step: 247, Loss: 0.016225701197981834\n",
      "Step: 248, Loss: 0.0029727257788181305\n",
      "Step: 249, Loss: 0.005320378579199314\n",
      "Step: 250, Loss: 0.0019388225628063083\n",
      "Step: 251, Loss: 0.004547582473605871\n",
      "Step: 252, Loss: 0.0016239499673247337\n",
      "Step: 253, Loss: 0.003052265616133809\n",
      "Step: 254, Loss: 0.0029648980125784874\n",
      "Step: 255, Loss: 0.0020829555578529835\n",
      "Step: 256, Loss: 0.004293498583137989\n",
      "Step: 257, Loss: 0.0037043129559606314\n",
      "Step: 258, Loss: 0.005210570991039276\n",
      "Step: 259, Loss: 0.002937817247584462\n",
      "Step: 260, Loss: 0.004057820420712233\n",
      "Step: 261, Loss: 0.0014622823800891638\n",
      "Step: 262, Loss: 0.01650065742433071\n",
      "Step: 263, Loss: 0.0024879323318600655\n",
      "Step: 264, Loss: 0.0018166654044762254\n",
      "Step: 265, Loss: 0.002021661028265953\n",
      "Step: 266, Loss: 0.0046365405432879925\n",
      "Step: 267, Loss: 0.001507997396402061\n",
      "Step: 268, Loss: 0.0016705117886886\n",
      "Step: 269, Loss: 0.0025240513496100903\n",
      "Step: 270, Loss: 0.00131527881603688\n",
      "Step: 271, Loss: 0.019630191847682\n",
      "Step: 272, Loss: 0.008024980314075947\n",
      "Step: 273, Loss: 0.003325448837131262\n",
      "Step: 274, Loss: 0.002644185209646821\n",
      "Step: 275, Loss: 0.00493239238858223\n",
      "Step: 276, Loss: 0.003451621625572443\n",
      "Step: 277, Loss: 0.005014889873564243\n",
      "Step: 278, Loss: 0.004201851319521666\n",
      "Step: 279, Loss: 0.0020602212753146887\n",
      "Step: 280, Loss: 0.003401444060727954\n",
      "Step: 281, Loss: 0.0036406840663403273\n",
      "Step: 282, Loss: 0.006241421680897474\n",
      "Step: 283, Loss: 0.0030561520252376795\n",
      "Step: 284, Loss: 0.003474910045042634\n",
      "Step: 285, Loss: 0.0022189454175531864\n",
      "Step: 286, Loss: 0.004915915429592133\n",
      "Step: 287, Loss: 0.0038167485035955906\n",
      "Step: 288, Loss: 0.013205715455114841\n",
      "Step: 289, Loss: 0.001768598915077746\n",
      "Step: 290, Loss: 0.002329854294657707\n",
      "Step: 291, Loss: 0.0020545467268675566\n",
      "Step: 292, Loss: 0.0011238174047321081\n",
      "Step: 293, Loss: 0.0909574031829834\n",
      "Step: 294, Loss: 0.0024703582748770714\n",
      "Step: 295, Loss: 0.0029645333997905254\n",
      "Step: 296, Loss: 0.002025146968662739\n",
      "Step: 297, Loss: 0.08457497507333755\n",
      "Step: 298, Loss: 0.0017610967624932528\n",
      "Step: 299, Loss: 0.010857313871383667\n",
      "Step: 300, Loss: 0.45635250210762024\n",
      "Step: 301, Loss: 0.002644735388457775\n",
      "Step: 302, Loss: 0.0016897051827982068\n",
      "Step: 303, Loss: 0.0073974356055259705\n",
      "Step: 304, Loss: 0.0028506743256002665\n",
      "Step: 305, Loss: 0.003258110722526908\n",
      "Step: 306, Loss: 0.001930111087858677\n",
      "Step: 307, Loss: 0.003611518070101738\n",
      "Step: 308, Loss: 0.0026111213956028223\n",
      "Step: 309, Loss: 0.004330081399530172\n",
      "Step: 310, Loss: 0.011570780538022518\n",
      "Step: 311, Loss: 0.16729113459587097\n",
      "Step: 312, Loss: 0.0021488440688699484\n",
      "Step: 313, Loss: 0.00184017070569098\n",
      "Step: 314, Loss: 0.0020156537648290396\n",
      "Step: 315, Loss: 0.0029704845510423183\n",
      "Step: 316, Loss: 0.16869404911994934\n",
      "Step: 317, Loss: 0.001379063120111823\n",
      "Step: 318, Loss: 0.0028257223311811686\n",
      "Step: 319, Loss: 0.0018558439332991838\n",
      "Step: 320, Loss: 0.004357082303613424\n",
      "Step: 321, Loss: 0.0015731932362541556\n",
      "Step: 322, Loss: 0.0037860760930925608\n",
      "Step: 323, Loss: 0.0017631079535931349\n",
      "Step: 324, Loss: 0.0021336704958230257\n",
      "Step: 325, Loss: 0.0021762328688055277\n",
      "Step: 326, Loss: 0.0012806546874344349\n",
      "Step: 327, Loss: 0.0009086189093068242\n",
      "Step: 328, Loss: 0.00378612382337451\n",
      "Step: 329, Loss: 0.003111808327957988\n",
      "Step: 330, Loss: 0.002042848616838455\n",
      "Step: 331, Loss: 0.012417438440024853\n",
      "Step: 332, Loss: 0.002313606906682253\n",
      "Step: 333, Loss: 0.002286638831719756\n",
      "Step: 334, Loss: 0.0033957648556679487\n",
      "Step: 335, Loss: 0.0012576727895066142\n",
      "Step: 336, Loss: 0.0024250117130577564\n",
      "Step: 337, Loss: 0.0019026023801416159\n",
      "Step: 338, Loss: 0.002620421117171645\n",
      "Step: 339, Loss: 0.004737359005957842\n",
      "Step: 340, Loss: 0.002613852033391595\n",
      "Step: 341, Loss: 0.0018271178705617785\n",
      "Step: 342, Loss: 0.002531982958316803\n",
      "Step: 343, Loss: 0.004467375110834837\n",
      "Step: 344, Loss: 0.004134966991841793\n",
      "Step: 345, Loss: 0.0019541720394045115\n",
      "Step: 346, Loss: 0.005139858461916447\n",
      "Step: 347, Loss: 0.0029508050065487623\n",
      "Step: 348, Loss: 0.0036266366951167583\n",
      "Step: 349, Loss: 0.0009621662902645767\n",
      "Step: 350, Loss: 0.002874394878745079\n",
      "Step: 351, Loss: 0.0026906365528702736\n",
      "Step: 352, Loss: 0.0016232506604865193\n",
      "Step: 353, Loss: 0.009968198835849762\n",
      "Step: 354, Loss: 0.4520357549190521\n",
      "Step: 355, Loss: 0.0035422216169536114\n",
      "Step: 356, Loss: 0.07271657884120941\n",
      "Step: 357, Loss: 0.0031202631071209908\n",
      "Step: 358, Loss: 0.01324306707829237\n",
      "Step: 359, Loss: 0.002204206306487322\n",
      "Step: 360, Loss: 0.01726604625582695\n",
      "Step: 361, Loss: 0.004709006752818823\n",
      "Step: 362, Loss: 0.011997311376035213\n",
      "Step: 363, Loss: 0.002274608239531517\n",
      "Step: 364, Loss: 0.006779549177736044\n",
      "Step: 365, Loss: 0.0021857176907360554\n",
      "Step: 366, Loss: 0.007396938744932413\n",
      "Step: 367, Loss: 0.002949401503428817\n",
      "Step: 368, Loss: 0.008148688822984695\n",
      "Step: 369, Loss: 0.0016816845163702965\n",
      "Step: 370, Loss: 0.004208408296108246\n",
      "Step: 371, Loss: 0.0023008729331195354\n",
      "Step: 372, Loss: 0.002433108864352107\n",
      "Step: 373, Loss: 0.002097942167893052\n",
      "Step: 374, Loss: 0.002016291255131364\n",
      "Step: 375, Loss: 0.0064226845279335976\n",
      "Step: 376, Loss: 0.0020112700294703245\n",
      "Step: 377, Loss: 0.0044251312501728535\n",
      "Step: 378, Loss: 0.004529871512204409\n",
      "Step: 379, Loss: 0.006199964787811041\n",
      "Step: 380, Loss: 0.005745678208768368\n",
      "Step: 381, Loss: 0.015381969511508942\n",
      "Step: 382, Loss: 0.0017942233243957162\n",
      "Step: 383, Loss: 0.000967683270573616\n",
      "Step: 384, Loss: 0.008010955527424812\n",
      "Step: 385, Loss: 0.0016021536430343986\n",
      "Step: 386, Loss: 0.001542393583804369\n",
      "Step: 387, Loss: 0.0018194400472566485\n",
      "Step: 388, Loss: 0.0015630796551704407\n",
      "Step: 389, Loss: 0.0024387817829847336\n",
      "Step: 390, Loss: 0.004361226689070463\n",
      "Step: 391, Loss: 0.0007913639419712126\n",
      "Step: 392, Loss: 0.0030884388834238052\n",
      "Step: 393, Loss: 0.0017854758771136403\n",
      "Step: 394, Loss: 0.0029620821587741375\n",
      "Step: 395, Loss: 0.003462371649220586\n",
      "Step: 396, Loss: 0.0029365841764956713\n",
      "Step: 397, Loss: 0.0855737179517746\n",
      "Step: 398, Loss: 0.0041990517638623714\n",
      "Step: 399, Loss: 0.0033027969766408205\n",
      "Step: 400, Loss: 0.006759748328477144\n",
      "Step: 401, Loss: 0.006675712298601866\n",
      "Step: 402, Loss: 0.002528825541958213\n",
      "Step: 403, Loss: 0.005806963890790939\n",
      "Step: 404, Loss: 0.004201569128781557\n",
      "Step: 405, Loss: 0.025716153904795647\n",
      "Step: 406, Loss: 0.005407213233411312\n",
      "Step: 407, Loss: 0.0035304485354572535\n",
      "Step: 408, Loss: 0.005648086778819561\n",
      "Step: 409, Loss: 0.001287194318138063\n",
      "Step: 410, Loss: 0.003217675955966115\n",
      "Step: 411, Loss: 0.002073540585115552\n",
      "Step: 412, Loss: 0.0035482312086969614\n",
      "Step: 413, Loss: 0.005786435212939978\n",
      "Step: 414, Loss: 0.003895937465131283\n",
      "Step: 415, Loss: 0.002205905271694064\n",
      "Step: 416, Loss: 0.006723423022776842\n",
      "Step: 417, Loss: 0.0026083325501531363\n",
      "Step: 418, Loss: 0.010397213511168957\n",
      "Step: 419, Loss: 0.003430528100579977\n",
      "Step: 420, Loss: 0.0037808569613844156\n",
      "Step: 421, Loss: 0.0019663721323013306\n",
      "Step: 422, Loss: 0.00199887459166348\n",
      "Step: 423, Loss: 0.0025569195859134197\n",
      "Step: 424, Loss: 0.001488019130192697\n",
      "Step: 425, Loss: 0.05722641199827194\n",
      "Step: 426, Loss: 0.001017257571220398\n",
      "Step: 427, Loss: 0.014185694977641106\n",
      "Step: 428, Loss: 0.2120429426431656\n",
      "Step: 429, Loss: 0.003321654163300991\n",
      "Step: 430, Loss: 0.002789069665595889\n",
      "Step: 431, Loss: 0.0036503460723906755\n",
      "Step: 432, Loss: 0.0021549256052821875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 433, Loss: 0.002127466257661581\n",
      "Step: 434, Loss: 0.0026110948529094458\n",
      "Step: 435, Loss: 0.0019226670265197754\n",
      "Step: 436, Loss: 0.0015028591733425856\n",
      "Step: 437, Loss: 0.001929509686306119\n",
      "Step: 438, Loss: 0.001659032772295177\n",
      "Step: 439, Loss: 0.0037272318731993437\n",
      "Step: 440, Loss: 0.0025619722437113523\n",
      "Step: 441, Loss: 0.0017431078013032675\n",
      "Step: 442, Loss: 0.0013750045327469707\n",
      "Step: 443, Loss: 0.0011509406613186002\n",
      "Step: 444, Loss: 0.002734076464548707\n",
      "Step: 445, Loss: 0.0033527042251080275\n",
      "Step: 446, Loss: 0.005888482555747032\n",
      "Step: 447, Loss: 0.0018963689217343926\n",
      "Step: 448, Loss: 0.0021056109108030796\n",
      "Step: 449, Loss: 0.01368943601846695\n",
      "Step: 450, Loss: 0.001033573062159121\n",
      "Step: 451, Loss: 0.005063413642346859\n",
      "Step: 452, Loss: 0.0016600197413936257\n",
      "Step: 453, Loss: 0.0016217303927987814\n",
      "Step: 454, Loss: 0.0017620268044993281\n",
      "Step: 455, Loss: 0.0018427094910293818\n",
      "Step: 456, Loss: 0.002968088025227189\n",
      "Step: 457, Loss: 0.0020572924986481667\n",
      "Step: 458, Loss: 0.003664040006697178\n",
      "Step: 459, Loss: 0.0029265624471008778\n",
      "Step: 460, Loss: 0.002765769138932228\n",
      "Step: 461, Loss: 0.004476616624742746\n",
      "Step: 462, Loss: 0.0017642541788518429\n",
      "Step: 463, Loss: 0.0018677969928830862\n",
      "Step: 464, Loss: 0.0022480087354779243\n",
      "Step: 465, Loss: 0.0015432193176820874\n",
      "Step: 466, Loss: 0.010736188851296902\n",
      "Step: 467, Loss: 0.002184974029660225\n",
      "Step: 468, Loss: 0.002861584769561887\n",
      "Step: 469, Loss: 0.011318675242364407\n",
      "Step: 470, Loss: 0.0051208846271038055\n",
      "Step: 471, Loss: 0.28575313091278076\n",
      "Step: 472, Loss: 0.0024516212288290262\n",
      "Step: 473, Loss: 0.0029380102641880512\n",
      "Step: 474, Loss: 0.002082398161292076\n",
      "Step: 475, Loss: 0.004320727661252022\n",
      "Step: 476, Loss: 0.005992400925606489\n",
      "Step: 477, Loss: 0.003863814752548933\n",
      "Step: 478, Loss: 0.0150087745860219\n",
      "Step: 479, Loss: 0.017190229147672653\n",
      "Step: 480, Loss: 0.004634005017578602\n",
      "Step: 481, Loss: 0.002450051950290799\n",
      "Step: 482, Loss: 0.0033560257870703936\n",
      "Step: 483, Loss: 0.0015874920645728707\n",
      "Step: 484, Loss: 0.0016367583302780986\n",
      "Step: 485, Loss: 0.0031197182834148407\n",
      "Step: 486, Loss: 0.004954105708748102\n",
      "Step: 487, Loss: 0.0023149210028350353\n",
      "Step: 488, Loss: 0.011452165432274342\n",
      "Step: 489, Loss: 0.0019261704292148352\n",
      "Step: 490, Loss: 0.0035593754146248102\n",
      "Step: 491, Loss: 0.0017547870520502329\n",
      "Step: 492, Loss: 0.0035242820158600807\n",
      "Step: 493, Loss: 0.019706420600414276\n",
      "Step: 494, Loss: 0.002162545220926404\n",
      "Step: 495, Loss: 0.001589830731973052\n",
      "Step: 496, Loss: 0.002309901174157858\n",
      "Step: 497, Loss: 0.0014583361335098743\n",
      "Step: 498, Loss: 0.0030043069273233414\n",
      "Step: 499, Loss: 0.0013589507434517145\n",
      "Step: 500, Loss: 0.002627243287861347\n",
      "Step: 501, Loss: 0.002232177183032036\n",
      "Step: 502, Loss: 0.010233909823000431\n",
      "Step: 503, Loss: 0.004413587506860495\n",
      "Step: 504, Loss: 0.006397421471774578\n",
      "Step: 505, Loss: 0.002833870705217123\n",
      "Step: 506, Loss: 0.0036650989204645157\n",
      "Step: 507, Loss: 0.003475771751254797\n",
      "Step: 508, Loss: 0.0022434615530073643\n",
      "Step: 509, Loss: 0.0024588059168308973\n",
      "Step: 510, Loss: 0.00342959794215858\n",
      "Step: 511, Loss: 0.0023810367565602064\n",
      "Step: 512, Loss: 0.003861339995637536\n",
      "Step: 513, Loss: 0.0023378441110253334\n",
      "Step: 514, Loss: 0.002358823549002409\n",
      "Step: 515, Loss: 0.0014039140660315752\n",
      "Step: 516, Loss: 0.0016481390921398997\n",
      "Step: 517, Loss: 0.0022063618525862694\n",
      "Step: 518, Loss: 0.000745687575545162\n",
      "Step: 519, Loss: 0.0038885578978806734\n",
      "Step: 520, Loss: 0.002672293921932578\n",
      "Step: 521, Loss: 0.0016492087161168456\n",
      "Step: 522, Loss: 0.002083483152091503\n",
      "Step: 523, Loss: 0.003158513456583023\n",
      "Step: 524, Loss: 0.001976852770894766\n",
      "Step: 525, Loss: 0.0034113472793251276\n",
      "Step: 526, Loss: 0.0017786365933716297\n",
      "Step: 527, Loss: 0.02510487474501133\n",
      "Step: 528, Loss: 0.003185796784237027\n",
      "Step: 529, Loss: 0.001027434947900474\n",
      "Step: 530, Loss: 0.008658000268042088\n",
      "Step: 531, Loss: 0.001203587045893073\n",
      "Step: 532, Loss: 0.003971561789512634\n",
      "Step: 533, Loss: 0.002174577210098505\n",
      "Step: 534, Loss: 0.0016041616909205914\n",
      "Step: 535, Loss: 0.002313140081241727\n",
      "Step: 536, Loss: 0.003997761756181717\n",
      "Step: 537, Loss: 0.01039086189121008\n",
      "Step: 538, Loss: 0.002254817634820938\n",
      "Step: 539, Loss: 0.003629712387919426\n",
      "Step: 540, Loss: 0.0027372317854315042\n",
      "Step: 541, Loss: 0.004139023367315531\n",
      "Step: 542, Loss: 0.0036313768941909075\n",
      "Step: 543, Loss: 0.0035701815504580736\n",
      "Step: 544, Loss: 0.0020418756175786257\n",
      "Step: 545, Loss: 0.04315030202269554\n",
      "Step: 546, Loss: 0.0027638578321784735\n",
      "Step: 547, Loss: 0.0012530455132946372\n",
      "Step: 548, Loss: 0.005659332498908043\n",
      "Step: 549, Loss: 0.004501737654209137\n",
      "Step: 550, Loss: 0.001637255190871656\n",
      "Step: 551, Loss: 0.005852182395756245\n",
      "Step: 552, Loss: 0.0022279941476881504\n",
      "Step: 553, Loss: 0.001614724867977202\n",
      "Step: 554, Loss: 0.012317635118961334\n",
      "Step: 555, Loss: 0.0021784589625895023\n",
      "Step: 556, Loss: 0.4490688443183899\n",
      "Step: 557, Loss: 0.00505826435983181\n",
      "Step: 558, Loss: 0.0012160277692601085\n",
      "Step: 559, Loss: 0.003312889952212572\n",
      "Step: 560, Loss: 0.0012065768241882324\n",
      "Step: 561, Loss: 0.0056365011259913445\n",
      "Step: 562, Loss: 0.0019264811417087913\n",
      "Step: 563, Loss: 0.0012595694279298186\n",
      "Step: 564, Loss: 0.009939506649971008\n",
      "Step: 565, Loss: 0.0022101711947470903\n",
      "Step: 566, Loss: 0.003796114819124341\n",
      "Step: 567, Loss: 0.002731519751250744\n",
      "Step: 568, Loss: 0.002776896581053734\n",
      "Step: 569, Loss: 0.002171139232814312\n",
      "Step: 570, Loss: 0.0017357178730890155\n",
      "Step: 571, Loss: 0.0017060618847608566\n",
      "Step: 572, Loss: 0.0018244123784825206\n",
      "Step: 573, Loss: 0.003165526781231165\n",
      "Step: 574, Loss: 0.007088359445333481\n",
      "Step: 575, Loss: 0.001582448254339397\n",
      "Step: 576, Loss: 0.0014615514082834125\n",
      "Step: 577, Loss: 0.008133561350405216\n",
      "Step: 578, Loss: 0.0017449920997023582\n",
      "Step: 579, Loss: 0.0021137078292667866\n",
      "Step: 580, Loss: 0.002753206994384527\n",
      "Step: 581, Loss: 0.004900400992482901\n",
      "Step: 582, Loss: 0.0008274462888948619\n",
      "Step: 583, Loss: 0.003848531749099493\n",
      "Step: 584, Loss: 0.0025125781539827585\n",
      "Step: 585, Loss: 0.002426388207823038\n",
      "Step: 586, Loss: 0.019794004037976265\n",
      "Step: 587, Loss: 0.0018674226012080908\n",
      "Step: 588, Loss: 0.005960224196314812\n",
      "Step: 589, Loss: 0.0040249451994895935\n",
      "Step: 590, Loss: 0.004517324734479189\n",
      "Step: 591, Loss: 0.0039142281748354435\n",
      "Step: 592, Loss: 0.004880633670836687\n",
      "Step: 593, Loss: 0.0033110722433775663\n",
      "Step: 594, Loss: 0.0018537245923653245\n",
      "Step: 595, Loss: 0.0029723942279815674\n",
      "Step: 596, Loss: 0.0029629948548972607\n",
      "Step: 597, Loss: 0.09549320489168167\n",
      "Step: 598, Loss: 0.023326652124524117\n",
      "Step: 599, Loss: 0.0018951689125970006\n",
      "Step: 600, Loss: 0.0021963343024253845\n",
      "Step: 601, Loss: 0.00604517525061965\n",
      "Step: 602, Loss: 0.0034719903487712145\n",
      "Step: 603, Loss: 0.006362386047840118\n",
      "Step: 604, Loss: 0.0019892477430403233\n",
      "Step: 605, Loss: 0.0017518526874482632\n",
      "Step: 606, Loss: 0.002631306881085038\n",
      "Step: 607, Loss: 0.07418181002140045\n",
      "Step: 608, Loss: 0.0016062031500041485\n",
      "Step: 609, Loss: 0.005527626723051071\n",
      "Step: 610, Loss: 0.002553823869675398\n",
      "Step: 611, Loss: 0.006906231865286827\n",
      "Step: 612, Loss: 0.009543142281472683\n",
      "Step: 613, Loss: 0.0012054643593728542\n",
      "Step: 614, Loss: 0.0016329082427546382\n",
      "Step: 615, Loss: 0.004856957122683525\n",
      "Step: 616, Loss: 0.1214645504951477\n",
      "Step: 617, Loss: 0.0027127298526465893\n",
      "Step: 618, Loss: 0.0030740131624042988\n",
      "Step: 619, Loss: 0.0016507119871675968\n",
      "Step: 620, Loss: 0.0015264364192262292\n",
      "Step: 621, Loss: 0.005769270472228527\n",
      "Step: 622, Loss: 0.09727192670106888\n",
      "Step: 623, Loss: 0.0018131460528820753\n",
      "Step: 624, Loss: 0.0017714746063575149\n",
      "Step: 625, Loss: 0.003509353380650282\n",
      "Step: 626, Loss: 0.005268700886517763\n",
      "Step: 627, Loss: 0.002134261652827263\n",
      "Step: 628, Loss: 0.0046558682806789875\n",
      "Step: 629, Loss: 0.002097618766129017\n",
      "Step: 630, Loss: 0.6647878885269165\n",
      "Step: 631, Loss: 0.0029971275944262743\n",
      "Step: 632, Loss: 0.003123653819784522\n",
      "Step: 633, Loss: 0.0022425339557230473\n",
      "Step: 634, Loss: 0.004325804300606251\n",
      "Step: 635, Loss: 0.02087453566491604\n",
      "Step: 636, Loss: 0.06622637063264847\n",
      "Step: 637, Loss: 0.00624490063637495\n",
      "Step: 638, Loss: 0.0022552725858986378\n",
      "Step: 639, Loss: 0.0028503069188445807\n",
      "Step: 640, Loss: 0.009297452867031097\n",
      "Step: 641, Loss: 0.0025627606082707644\n",
      "Step: 642, Loss: 0.002549099503085017\n",
      "Step: 643, Loss: 0.00668086064979434\n",
      "Step: 644, Loss: 0.006875711493194103\n",
      "Step: 645, Loss: 0.0013858865713700652\n",
      "Step: 646, Loss: 0.07610545307397842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 647, Loss: 0.006666482426226139\n",
      "Step: 648, Loss: 0.0025270399637520313\n",
      "Step: 649, Loss: 0.004453642293810844\n",
      "Step: 650, Loss: 0.004256545100361109\n",
      "Step: 651, Loss: 0.01574643701314926\n",
      "Step: 652, Loss: 0.0023656131234019995\n",
      "Step: 653, Loss: 0.0030327914282679558\n",
      "Step: 654, Loss: 0.005447061266750097\n",
      "Step: 655, Loss: 0.002076281001791358\n",
      "Step: 656, Loss: 0.0022484406363219023\n",
      "Step: 657, Loss: 0.0026733926497399807\n",
      "Step: 658, Loss: 0.003824852406978607\n",
      "Step: 659, Loss: 0.0026995453517884016\n",
      "Step: 660, Loss: 0.009996505454182625\n",
      "Step: 661, Loss: 0.002824068535119295\n",
      "Step: 662, Loss: 0.6676353812217712\n",
      "Step: 663, Loss: 0.0020687321666628122\n",
      "Step: 664, Loss: 0.0025507179088890553\n",
      "Step: 665, Loss: 0.0013710736529901624\n",
      "Step: 666, Loss: 0.002215739106759429\n",
      "Step: 667, Loss: 0.0015895103570073843\n",
      "Step: 668, Loss: 0.03982822597026825\n",
      "Step: 669, Loss: 0.0028478107415139675\n",
      "Step: 670, Loss: 0.0015196963213384151\n",
      "Step: 671, Loss: 0.005132897756993771\n",
      "Step: 672, Loss: 0.0017046985449269414\n",
      "Step: 673, Loss: 0.001517977099865675\n",
      "Step: 674, Loss: 0.002135111950337887\n",
      "Step: 675, Loss: 0.0010995448101311922\n",
      "Step: 676, Loss: 0.013982110656797886\n",
      "Step: 677, Loss: 0.008218120783567429\n",
      "Step: 678, Loss: 0.012469976209104061\n",
      "Step: 679, Loss: 0.029911799356341362\n",
      "Step: 680, Loss: 0.002441421151161194\n",
      "Step: 681, Loss: 0.0039947498589754105\n",
      "Step: 682, Loss: 0.003937125205993652\n",
      "Step: 683, Loss: 0.003955242224037647\n",
      "Step: 684, Loss: 0.003036252921447158\n",
      "Step: 685, Loss: 0.0037249766755849123\n",
      "Step: 686, Loss: 0.001183716463856399\n",
      "Step: 687, Loss: 0.0018338122172281146\n",
      "Step: 688, Loss: 0.003522635903209448\n",
      "Step: 689, Loss: 0.002709914930164814\n",
      "Step: 690, Loss: 0.002074850257486105\n",
      "Step: 691, Loss: 0.01650727167725563\n",
      "Step: 692, Loss: 0.0014304423239082098\n",
      "Step: 693, Loss: 0.002135873306542635\n",
      "Step: 694, Loss: 0.005775813013315201\n",
      "Step: 695, Loss: 0.0015833501238375902\n",
      "Step: 696, Loss: 0.0017164329765364528\n",
      "Step: 697, Loss: 0.0019501576898619533\n",
      "Step: 698, Loss: 0.006703635212033987\n",
      "Step: 699, Loss: 0.006284263916313648\n",
      "Step: 700, Loss: 0.008470970205962658\n",
      "Step: 701, Loss: 0.0039133247919380665\n",
      "Step: 702, Loss: 0.003233259078115225\n",
      "Step: 703, Loss: 0.0022856267169117928\n",
      "Step: 704, Loss: 0.018466884270310402\n",
      "Step: 705, Loss: 0.006429826840758324\n",
      "Step: 706, Loss: 0.0011536715319380164\n",
      "Step: 707, Loss: 0.017004363238811493\n",
      "Step: 708, Loss: 0.0018044940661638975\n",
      "Step: 709, Loss: 0.001392980688251555\n",
      "Step: 710, Loss: 0.0033010942861437798\n",
      "Step: 711, Loss: 0.00935899093747139\n",
      "Step: 712, Loss: 0.0023575229570269585\n",
      "Step: 713, Loss: 0.0025352565571665764\n",
      "Step: 714, Loss: 0.049000538885593414\n",
      "Step: 715, Loss: 0.0018690414726734161\n",
      "Step: 716, Loss: 0.001577821560204029\n",
      "Step: 717, Loss: 0.001719383173622191\n",
      "Step: 718, Loss: 0.0690392553806305\n",
      "Step: 719, Loss: 0.0028862101025879383\n",
      "Step: 720, Loss: 0.0018458014819771051\n",
      "Step: 721, Loss: 0.006923025473952293\n",
      "Step: 722, Loss: 0.0021137672010809183\n",
      "Step: 723, Loss: 0.0021171316038817167\n",
      "Step: 724, Loss: 0.0053869751282036304\n",
      "Step: 725, Loss: 0.002672723960131407\n",
      "Step: 726, Loss: 0.003439644817262888\n",
      "Step: 727, Loss: 0.0023676094133406878\n",
      "Step: 728, Loss: 0.012551193125545979\n",
      "Step: 729, Loss: 0.02009277604520321\n",
      "Step: 730, Loss: 0.001915080239996314\n",
      "Step: 731, Loss: 0.0014228832442313433\n",
      "Step: 732, Loss: 0.012591784819960594\n",
      "Step: 733, Loss: 0.002477480797097087\n",
      "Step: 734, Loss: 1.1955584287643433\n",
      "Step: 735, Loss: 0.0024061878211796284\n",
      "Step: 736, Loss: 0.002334858290851116\n",
      "Step: 737, Loss: 0.009156403131783009\n",
      "Step: 738, Loss: 0.0036562285386025906\n",
      "Step: 739, Loss: 0.0026947956066578627\n",
      "Step: 740, Loss: 0.008908355608582497\n",
      "Step: 741, Loss: 0.0047047436237335205\n",
      "Step: 742, Loss: 0.0015402059070765972\n",
      "Step: 743, Loss: 0.00195408146828413\n",
      "Step: 744, Loss: 0.002950199879705906\n",
      "Step: 745, Loss: 0.012971553951501846\n",
      "Step: 746, Loss: 0.3986840546131134\n",
      "Step: 747, Loss: 0.0025253945495933294\n",
      "Step: 748, Loss: 0.002401951467618346\n",
      "Step: 749, Loss: 0.002758969319984317\n",
      "Step: 750, Loss: 0.0033658239990472794\n",
      "Step: 751, Loss: 0.004075770732015371\n",
      "Step: 752, Loss: 0.0026424413081258535\n",
      "Step: 753, Loss: 0.0053185769356787205\n",
      "Step: 754, Loss: 0.0020272736437618732\n",
      "Step: 755, Loss: 0.004368423949927092\n",
      "Step: 756, Loss: 0.0014270448591560125\n",
      "Step: 757, Loss: 0.00539690675213933\n",
      "Step: 758, Loss: 0.0022718762047588825\n",
      "Step: 759, Loss: 0.003216756274923682\n",
      "Step: 760, Loss: 0.002419057535007596\n",
      "Step: 761, Loss: 0.0025611533783376217\n",
      "Step: 762, Loss: 0.0020670921076089144\n",
      "Step: 763, Loss: 0.0015866836765781045\n",
      "Step: 764, Loss: 0.2948140799999237\n",
      "Step: 765, Loss: 0.011467983946204185\n",
      "Step: 766, Loss: 0.001462114742025733\n",
      "Step: 767, Loss: 0.0017838651547208428\n",
      "Step: 768, Loss: 0.02332494594156742\n",
      "Step: 769, Loss: 0.003974419552832842\n",
      "Step: 770, Loss: 0.0034304452128708363\n",
      "Step: 771, Loss: 0.0056212469935417175\n",
      "Step: 772, Loss: 0.09956088662147522\n",
      "Step: 773, Loss: 0.002015703823417425\n",
      "Step: 774, Loss: 0.007656314875930548\n",
      "Step: 775, Loss: 0.002214602893218398\n",
      "Step: 776, Loss: 0.004061630927026272\n",
      "Step: 777, Loss: 0.0033122901804745197\n",
      "Step: 778, Loss: 0.034214556217193604\n",
      "Step: 779, Loss: 0.0026784129440784454\n",
      "Step: 780, Loss: 0.0045962342992424965\n",
      "Step: 781, Loss: 0.005119411274790764\n",
      "Step: 782, Loss: 0.0038920422084629536\n",
      "Step: 783, Loss: 0.0037285713478922844\n",
      "Step: 784, Loss: 0.002791927196085453\n",
      "Step: 785, Loss: 0.0029141055420041084\n",
      "Step: 786, Loss: 0.0017599182901903987\n",
      "Step: 787, Loss: 0.0014052509795874357\n",
      "Step: 788, Loss: 0.005347196012735367\n",
      "Step: 789, Loss: 0.0024309700820595026\n",
      "Step: 790, Loss: 0.0021912679076194763\n",
      "Step: 791, Loss: 0.38243919610977173\n",
      "Step: 792, Loss: 0.0019027921371161938\n",
      "Step: 793, Loss: 0.008531900122761726\n",
      "Step: 794, Loss: 0.023073608055710793\n",
      "Step: 795, Loss: 0.00599644286558032\n",
      "Step: 796, Loss: 0.01235865242779255\n",
      "Step: 797, Loss: 0.0022348954807966948\n",
      "Step: 798, Loss: 0.0024237697944045067\n",
      "Step: 799, Loss: 0.0018622823990881443\n",
      "Step: 800, Loss: 0.0018126360373571515\n",
      "Step: 801, Loss: 0.0026707686483860016\n",
      "Step: 802, Loss: 0.0015018736012279987\n",
      "Step: 803, Loss: 0.01222458016127348\n",
      "Step: 804, Loss: 0.0020269628148525953\n",
      "Step: 805, Loss: 0.005328711587935686\n",
      "Step: 806, Loss: 0.0027653237339109182\n",
      "Step: 807, Loss: 0.0020454740151762962\n",
      "Step: 808, Loss: 0.002988457912579179\n",
      "Step: 809, Loss: 0.002974824048578739\n",
      "Step: 810, Loss: 0.0027444614097476006\n",
      "Step: 811, Loss: 0.05381833761930466\n",
      "Step: 812, Loss: 0.003258564742282033\n",
      "Step: 813, Loss: 0.0017504083225503564\n",
      "Step: 814, Loss: 0.002209713449701667\n",
      "Step: 815, Loss: 0.0071661039255559444\n",
      "Step: 816, Loss: 0.0026611769571900368\n",
      "Step: 817, Loss: 0.0017383802914991975\n",
      "Step: 818, Loss: 0.007229974959045649\n",
      "Step: 819, Loss: 0.0020143266301602125\n",
      "Step: 820, Loss: 0.002267436357215047\n",
      "Step: 821, Loss: 0.00251225964166224\n",
      "Step: 824, Loss: 0.002325564855709672\n",
      "Step: 825, Loss: 0.007481186650693417\n",
      "Step: 826, Loss: 0.00395515700802207\n",
      "Step: 827, Loss: 0.002413613721728325\n",
      "Step: 828, Loss: 0.0019523226656019688\n",
      "Step: 829, Loss: 0.0020351626444607973\n",
      "Step: 830, Loss: 0.0018375397194176912\n",
      "Step: 831, Loss: 0.0013488177210092545\n",
      "Step: 832, Loss: 0.0018287451239302754\n",
      "Step: 833, Loss: 0.001335127861239016\n",
      "Step: 834, Loss: 0.001555638387799263\n",
      "Step: 835, Loss: 0.002021925523877144\n",
      "Step: 836, Loss: 0.0022573648020625114\n",
      "Step: 837, Loss: 0.004049133509397507\n",
      "Step: 838, Loss: 0.0016318083507940173\n",
      "Step: 839, Loss: 0.0009647880797274411\n",
      "Step: 840, Loss: 0.002554885111749172\n",
      "Step: 841, Loss: 0.0237560011446476\n",
      "Step: 842, Loss: 0.0036041790153831244\n",
      "Step: 843, Loss: 0.0025335701648145914\n",
      "Step: 844, Loss: 0.0010188596788793802\n",
      "Step: 845, Loss: 0.011233992874622345\n",
      "Step: 846, Loss: 0.0026557862292975187\n",
      "Step: 847, Loss: 0.0013985391706228256\n",
      "Step: 848, Loss: 0.004221503622829914\n",
      "Step: 849, Loss: 0.07315275073051453\n",
      "Step: 850, Loss: 0.00222981721162796\n",
      "Step: 851, Loss: 0.0033391129691153765\n",
      "Step: 852, Loss: 0.0018219517078250647\n",
      "Step: 853, Loss: 0.0014456333592534065\n",
      "Step: 854, Loss: 0.002998829586431384\n",
      "Step: 855, Loss: 0.0017192085506394506\n",
      "Step: 856, Loss: 0.0012905823532491922\n",
      "Step: 857, Loss: 0.0025753863155841827\n",
      "Step: 858, Loss: 0.1540466994047165\n",
      "Step: 859, Loss: 0.0023034755140542984\n",
      "Step: 860, Loss: 0.0032649519853293896\n",
      "Step: 861, Loss: 0.003678571665659547\n",
      "Step: 862, Loss: 0.008910957723855972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 863, Loss: 0.005882874131202698\n",
      "Step: 864, Loss: 0.017213795334100723\n",
      "Step: 865, Loss: 0.0033872416242957115\n",
      "Step: 866, Loss: 0.0030763663817197084\n",
      "Step: 867, Loss: 0.002657501492649317\n",
      "Step: 868, Loss: 0.0018184693763032556\n",
      "Step: 869, Loss: 0.0022097681649029255\n",
      "Step: 870, Loss: 0.005786169320344925\n",
      "Step: 871, Loss: 0.003313716035336256\n",
      "Step: 872, Loss: 0.003695148741826415\n",
      "Step: 873, Loss: 0.0008768431725911796\n",
      "Step: 874, Loss: 0.0015410714549943805\n",
      "Step: 875, Loss: 0.002835009014233947\n",
      "Step: 876, Loss: 0.0039508892223238945\n",
      "Step: 877, Loss: 0.0015872716903686523\n",
      "Step: 878, Loss: 0.010298576205968857\n",
      "Step: 879, Loss: 0.0014471702743321657\n",
      "Step: 880, Loss: 0.003709024516865611\n",
      "Step: 881, Loss: 0.0018247028347104788\n",
      "Step: 882, Loss: 0.002932900795713067\n",
      "Step: 883, Loss: 0.0011614125687628984\n",
      "Step: 884, Loss: 0.5195192098617554\n",
      "Step: 885, Loss: 0.0015356983058154583\n",
      "Step: 886, Loss: 0.0036173537373542786\n",
      "Step: 887, Loss: 0.0016477127792313695\n",
      "Step: 888, Loss: 0.005462032277137041\n",
      "Step: 889, Loss: 0.0014242623001337051\n",
      "Step: 890, Loss: 0.0015995465219020844\n",
      "Step: 891, Loss: 0.02491592988371849\n",
      "Step: 892, Loss: 0.0021230445709079504\n",
      "Step: 893, Loss: 0.004723012447357178\n",
      "Step: 894, Loss: 0.0031408919021487236\n",
      "Step: 895, Loss: 0.0013300657738000154\n",
      "Step: 896, Loss: 0.001235086703673005\n",
      "Step: 897, Loss: 0.0026620072312653065\n",
      "Step: 898, Loss: 0.005202649161219597\n",
      "Step: 899, Loss: 0.0016132050659507513\n",
      "Step: 900, Loss: 0.0017386648105457425\n",
      "Step: 901, Loss: 0.0029150922782719135\n",
      "Step: 902, Loss: 0.1505226194858551\n",
      "Step: 903, Loss: 0.004629729315638542\n",
      "Step: 904, Loss: 0.002156006870791316\n",
      "Step: 905, Loss: 0.00280470447614789\n",
      "Step: 906, Loss: 0.0014132634969428182\n",
      "Step: 907, Loss: 0.0029633224476128817\n",
      "Step: 908, Loss: 0.0021384679712355137\n",
      "Step: 909, Loss: 0.0020690327510237694\n",
      "Step: 910, Loss: 0.011974012479186058\n",
      "Step: 911, Loss: 0.0018048190977424383\n",
      "Step: 912, Loss: 0.0023070143070071936\n",
      "Step: 913, Loss: 0.0020518959499895573\n",
      "Step: 914, Loss: 0.002306209644302726\n",
      "Step: 915, Loss: 0.001961821224540472\n",
      "Step: 916, Loss: 0.002436243463307619\n",
      "Step: 917, Loss: 0.005831534042954445\n",
      "Step: 918, Loss: 0.0022183905821293592\n",
      "Step: 919, Loss: 0.0018390771001577377\n",
      "Step: 920, Loss: 0.002167911035940051\n",
      "Step: 921, Loss: 0.0017778703477233648\n",
      "Step: 922, Loss: 0.03825982287526131\n",
      "Step: 923, Loss: 0.001034609624184668\n",
      "Step: 924, Loss: 0.0035961726680397987\n",
      "Step: 925, Loss: 0.0010191441979259253\n",
      "Step: 926, Loss: 0.004241598304361105\n",
      "Step: 927, Loss: 0.0015659319469705224\n",
      "Step: 928, Loss: 0.0017609222559258342\n",
      "Step: 929, Loss: 0.0019790150690823793\n",
      "Step: 930, Loss: 0.002152452478185296\n",
      "Step: 931, Loss: 0.007649856153875589\n",
      "Step: 932, Loss: 0.0027799629606306553\n",
      "Step: 933, Loss: 0.0015949090011417866\n",
      "Step: 934, Loss: 0.0042861890979111195\n",
      "Step: 935, Loss: 0.0029312085825949907\n",
      "Step: 936, Loss: 0.0014113488141447306\n",
      "Step: 937, Loss: 0.003175081918016076\n",
      "Step: 938, Loss: 0.003045877441763878\n",
      "Step: 939, Loss: 0.002450647298246622\n",
      "Step: 940, Loss: 0.0036652456037700176\n",
      "Step: 941, Loss: 0.0021148482337594032\n",
      "Step: 942, Loss: 0.0038575809448957443\n",
      "Step: 943, Loss: 0.002562400419265032\n",
      "Step: 944, Loss: 0.011521953158080578\n",
      "Step: 945, Loss: 0.0029297340661287308\n",
      "Step: 946, Loss: 0.0026403339579701424\n",
      "Step: 947, Loss: 0.0024276506155729294\n",
      "Step: 948, Loss: 0.0022047695238143206\n",
      "Step: 949, Loss: 0.013826782815158367\n",
      "Step: 950, Loss: 0.0014247255166992545\n",
      "Step: 951, Loss: 0.0012239634525030851\n",
      "Step: 952, Loss: 0.0014468168374150991\n",
      "Step: 953, Loss: 0.00468374602496624\n",
      "Step: 954, Loss: 0.0037127446848899126\n",
      "Step: 955, Loss: 0.002099973149597645\n",
      "Step: 956, Loss: 0.0020760854240506887\n",
      "Step: 957, Loss: 0.003612568136304617\n",
      "Step: 958, Loss: 0.0019008027156814933\n",
      "Step: 959, Loss: 0.0015764059498906136\n",
      "Step: 960, Loss: 0.0017173453234136105\n",
      "Step: 961, Loss: 0.003326980397105217\n",
      "Step: 962, Loss: 0.9339015483856201\n",
      "Step: 963, Loss: 0.0023439503274858\n",
      "Step: 964, Loss: 0.0019339433638378978\n",
      "Step: 965, Loss: 0.004051143769174814\n",
      "Step: 966, Loss: 0.004246192052960396\n",
      "Step: 967, Loss: 0.002715205540880561\n",
      "Step: 968, Loss: 0.0031372623052448034\n",
      "Step: 969, Loss: 0.006686556152999401\n",
      "Step: 970, Loss: 0.004169486928731203\n",
      "Step: 971, Loss: 0.0024263246450573206\n",
      "Step: 972, Loss: 0.01869635097682476\n",
      "Step: 973, Loss: 0.003572259796783328\n",
      "Step: 974, Loss: 0.015023324638605118\n",
      "Step: 975, Loss: 0.0017990480409935117\n",
      "Step: 976, Loss: 0.0017982893623411655\n",
      "Step: 977, Loss: 0.0020743582863360643\n",
      "Step: 978, Loss: 0.017733808606863022\n",
      "Step: 979, Loss: 0.0028868019580841064\n",
      "Step: 980, Loss: 0.005700934212654829\n",
      "Step: 981, Loss: 0.0035723403561860323\n",
      "Step: 982, Loss: 0.0011745465453714132\n",
      "Step: 983, Loss: 0.0038903725799173117\n",
      "Step: 984, Loss: 0.009196456521749496\n",
      "Step: 985, Loss: 0.003480667481198907\n",
      "Step: 986, Loss: 0.0013776503037661314\n",
      "Step: 987, Loss: 0.0023079244419932365\n",
      "Step: 988, Loss: 0.0027239592745900154\n",
      "Step: 989, Loss: 0.006588953081518412\n",
      "Step: 990, Loss: 0.03554118052124977\n",
      "Step: 991, Loss: 0.04667137190699577\n",
      "Step: 992, Loss: 0.003312109038233757\n",
      "Step: 993, Loss: 0.002698334399610758\n",
      "Step: 994, Loss: 0.003138355677947402\n",
      "Step: 995, Loss: 0.022663405165076256\n",
      "Step: 996, Loss: 0.013976086862385273\n",
      "Step: 997, Loss: 0.0029136117082089186\n",
      "Step: 998, Loss: 0.005847907159477472\n",
      "Step: 999, Loss: 0.003863824997097254\n",
      "Step: 1000, Loss: 0.004230183083564043\n",
      "Step: 1001, Loss: 0.004298698622733355\n",
      "Step: 1002, Loss: 0.0023250363301485777\n",
      "Step: 1003, Loss: 0.006736477371305227\n",
      "Step: 1004, Loss: 0.013858512043952942\n",
      "Step: 1005, Loss: 0.0013048360124230385\n",
      "Step: 1006, Loss: 0.002712251152843237\n",
      "Step: 1007, Loss: 0.0392807237803936\n",
      "Step: 1008, Loss: 0.002140502445399761\n",
      "Step: 1009, Loss: 0.013837550766766071\n",
      "Step: 1010, Loss: 0.003893817774951458\n",
      "Step: 1011, Loss: 0.02048531174659729\n",
      "Step: 1012, Loss: 0.0020257725846022367\n",
      "Step: 1013, Loss: 0.014446915127336979\n",
      "Step: 1014, Loss: 0.007442881353199482\n",
      "Step: 1015, Loss: 0.03324883058667183\n",
      "Step: 1016, Loss: 0.0016274589579552412\n",
      "Step: 1017, Loss: 0.007866417989134789\n",
      "Step: 1018, Loss: 0.002791760489344597\n",
      "Step: 1019, Loss: 0.0028432966209948063\n",
      "Step: 1020, Loss: 0.4670467972755432\n",
      "Step: 1021, Loss: 0.0027808984741568565\n",
      "Step: 1022, Loss: 0.001580244512297213\n",
      "Step: 1023, Loss: 0.009023749269545078\n",
      "Step: 1024, Loss: 0.004866443108767271\n",
      "Step: 1025, Loss: 0.0033073811791837215\n",
      "Step: 1026, Loss: 0.001365689910016954\n",
      "Step: 1027, Loss: 0.002142268233001232\n",
      "Step: 1028, Loss: 0.00445260526612401\n",
      "Step: 1029, Loss: 0.005392767488956451\n",
      "Step: 1030, Loss: 0.017145561054348946\n",
      "Step: 1031, Loss: 0.1788572072982788\n",
      "Step: 1032, Loss: 0.004277355968952179\n",
      "Step: 1033, Loss: 0.0014985227026045322\n",
      "Step: 1034, Loss: 0.016381852328777313\n",
      "Step: 1035, Loss: 0.0018483649473637342\n",
      "Step: 1036, Loss: 0.0023929239250719547\n",
      "Step: 1037, Loss: 0.0023185517638921738\n",
      "Step: 1038, Loss: 0.003401742083951831\n",
      "Step: 1039, Loss: 0.011361660435795784\n",
      "Step: 1040, Loss: 0.00197233771905303\n",
      "Step: 1041, Loss: 0.1555386632680893\n",
      "Step: 1042, Loss: 0.0020281814504414797\n",
      "Step: 1043, Loss: 0.002123237820342183\n",
      "Step: 1044, Loss: 0.01168595626950264\n",
      "Step: 1045, Loss: 0.006601223722100258\n",
      "Step: 1046, Loss: 0.009440976195037365\n",
      "Step: 1047, Loss: 0.04649130254983902\n",
      "Step: 1048, Loss: 0.002356806071475148\n",
      "Step: 1049, Loss: 0.01032193098217249\n",
      "Step: 1050, Loss: 0.008936965838074684\n",
      "Step: 1051, Loss: 0.0026044787373393774\n",
      "Step: 1052, Loss: 0.004152313806116581\n",
      "Step: 1053, Loss: 0.003158821491524577\n",
      "Step: 1054, Loss: 0.004538815934211016\n",
      "Step: 1055, Loss: 0.0023667518980801105\n",
      "Step: 1056, Loss: 0.002441684016957879\n",
      "Step: 1057, Loss: 0.002237192587926984\n",
      "Step: 1058, Loss: 0.0018354806816205382\n",
      "Step: 1059, Loss: 0.002102842554450035\n",
      "Step: 1060, Loss: 0.0023872668389230967\n",
      "Step: 1061, Loss: 0.003016976872459054\n",
      "Step: 1062, Loss: 0.0016525185201317072\n",
      "Step: 1063, Loss: 0.6899040937423706\n",
      "Step: 1064, Loss: 0.002033730736002326\n",
      "Step: 1065, Loss: 0.0029245999176055193\n",
      "Step: 1066, Loss: 0.006747380830347538\n",
      "Step: 1067, Loss: 0.4713189899921417\n",
      "Step: 1068, Loss: 0.002229891484603286\n",
      "Step: 1069, Loss: 0.00221769860945642\n",
      "Step: 1070, Loss: 0.0076160309836268425\n",
      "Step: 1071, Loss: 0.00354138039983809\n",
      "Step: 1072, Loss: 0.005155318882316351\n",
      "Step: 1073, Loss: 0.018008220940828323\n",
      "Step: 1074, Loss: 0.004133129492402077\n",
      "Step: 1075, Loss: 0.006275759544223547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1076, Loss: 0.001256974064745009\n",
      "Step: 1077, Loss: 0.0019898093305528164\n",
      "Step: 1078, Loss: 0.001930732629261911\n",
      "Step: 1079, Loss: 0.0028684709686785936\n",
      "Step: 1080, Loss: 0.0011741770431399345\n",
      "Step: 1081, Loss: 0.0020197804551571608\n",
      "Step: 1082, Loss: 0.01785127818584442\n",
      "Step: 1083, Loss: 0.0055684372782707214\n",
      "Step: 1084, Loss: 0.0012515343260020018\n",
      "Step: 1085, Loss: 0.0035046543926000595\n",
      "Step: 1086, Loss: 0.0023112508933991194\n",
      "Step: 1087, Loss: 0.0019823461771011353\n",
      "Step: 1088, Loss: 0.010432117618620396\n",
      "Step: 1089, Loss: 0.002022186294198036\n",
      "Step: 1090, Loss: 0.0011901756515726447\n",
      "Step: 1091, Loss: 0.0019732255022972822\n",
      "Step: 1092, Loss: 0.0027454604860395193\n",
      "Step: 1093, Loss: 0.002531148260459304\n",
      "Step: 1094, Loss: 0.006153766065835953\n",
      "Step: 1095, Loss: 0.0034583737142384052\n",
      "Step: 1096, Loss: 0.0019524915842339396\n",
      "Step: 1097, Loss: 0.0011839530197903514\n",
      "Step: 1098, Loss: 0.005288141779601574\n",
      "Step: 1099, Loss: 0.003302610944956541\n",
      "Step: 1100, Loss: 0.0019994049798697233\n",
      "Step: 1101, Loss: 0.0022078666370362043\n",
      "Step: 1102, Loss: 0.0013068427797406912\n",
      "Step: 1103, Loss: 0.002581452950835228\n",
      "Step: 1104, Loss: 0.004334145225584507\n",
      "Step: 1105, Loss: 0.0026053504552692175\n",
      "Step: 1106, Loss: 0.001869051600806415\n",
      "Step: 1107, Loss: 0.0045216865837574005\n",
      "Step: 1108, Loss: 0.004862446337938309\n",
      "Step: 1109, Loss: 0.011529913172125816\n",
      "Step: 1110, Loss: 0.003685012925416231\n",
      "Step: 1111, Loss: 0.0025622090324759483\n",
      "Step: 1112, Loss: 0.002551970537751913\n",
      "Step: 1113, Loss: 0.23434747755527496\n",
      "Step: 1114, Loss: 0.0011031200410798192\n",
      "Step: 1115, Loss: 0.00761738745495677\n",
      "Step: 1116, Loss: 0.05947431921958923\n",
      "Step: 1117, Loss: 0.0017264659982174635\n",
      "Step: 1118, Loss: 0.0024025607854127884\n",
      "Step: 1119, Loss: 0.008240574039518833\n",
      "Step: 1120, Loss: 0.0068506174720823765\n",
      "Step: 1121, Loss: 0.003362317569553852\n",
      "Step: 1122, Loss: 0.0601702518761158\n",
      "Step: 1123, Loss: 0.0012507784413173795\n",
      "Step: 1124, Loss: 0.0018982139881700277\n",
      "Step: 1125, Loss: 0.0015920549631118774\n",
      "Step: 1126, Loss: 0.0013170867459848523\n",
      "Step: 1127, Loss: 0.0017893408657982945\n",
      "Step: 1128, Loss: 0.0021709210705012083\n",
      "Step: 1129, Loss: 0.0024524007458239794\n",
      "Step: 1130, Loss: 0.003897467628121376\n",
      "Step: 1131, Loss: 0.00134854088537395\n",
      "Step: 1132, Loss: 0.001966834533959627\n",
      "Step: 1133, Loss: 0.00402803486213088\n",
      "Step: 1134, Loss: 0.00798848271369934\n",
      "Step: 1135, Loss: 0.008580544963479042\n",
      "Step: 1136, Loss: 0.00194858992472291\n",
      "Step: 1137, Loss: 0.012957580387592316\n",
      "Step: 1138, Loss: 0.004171764478087425\n",
      "Step: 1139, Loss: 0.0024945586919784546\n",
      "Step: 1140, Loss: 0.0075365393422544\n",
      "Step: 1141, Loss: 0.0021899729035794735\n",
      "Step: 1142, Loss: 0.002136607188731432\n",
      "Step: 1143, Loss: 0.0019490913255140185\n",
      "Step: 1144, Loss: 0.0023580759298056364\n",
      "Step: 1145, Loss: 0.0020392141304910183\n",
      "Step: 1146, Loss: 0.052999235689640045\n",
      "Step: 1147, Loss: 0.007757949642837048\n",
      "Step: 1148, Loss: 0.0011502048000693321\n",
      "Step: 1149, Loss: 0.0021116819698363543\n",
      "Step: 1150, Loss: 0.004081051331013441\n",
      "Step: 1151, Loss: 0.0011939743999391794\n",
      "Step: 1152, Loss: 0.0021766729187220335\n",
      "Step: 1153, Loss: 0.004643755499273539\n",
      "Step: 1154, Loss: 0.0017170674400404096\n",
      "Step: 1155, Loss: 0.0013187769800424576\n",
      "Step: 1156, Loss: 0.0038700310979038477\n",
      "Step: 1157, Loss: 0.3116755187511444\n",
      "Step: 1158, Loss: 0.002177707850933075\n",
      "Step: 1159, Loss: 0.00202145054936409\n",
      "Step: 1160, Loss: 0.005926237907260656\n",
      "Step: 1161, Loss: 0.0017577301478013396\n",
      "Step: 1162, Loss: 0.0038981882389634848\n",
      "Step: 1163, Loss: 0.0014614210231229663\n",
      "Step: 1164, Loss: 0.005369011778384447\n",
      "Step: 1165, Loss: 0.0017304378561675549\n",
      "Step: 1166, Loss: 0.0038858402986079454\n",
      "Step: 1167, Loss: 0.4160405993461609\n",
      "Step: 1168, Loss: 0.004596445243805647\n",
      "Step: 1169, Loss: 0.0015063128666952252\n",
      "Step: 1170, Loss: 0.0036391776520758867\n",
      "Step: 1171, Loss: 0.0016685507725924253\n",
      "Step: 1172, Loss: 0.0018293284811079502\n",
      "Step: 1173, Loss: 0.0017797426553443074\n",
      "Step: 1174, Loss: 0.0027739577926695347\n",
      "Step: 1175, Loss: 0.0028816249687224627\n",
      "Step: 1176, Loss: 0.0025453518610447645\n",
      "Step: 1177, Loss: 0.003169088624417782\n",
      "Step: 1178, Loss: 0.0017556268721818924\n",
      "Step: 1179, Loss: 0.001605039811693132\n",
      "Step: 1180, Loss: 0.001464189845137298\n",
      "Step: 1181, Loss: 0.030486544594168663\n",
      "Step: 1182, Loss: 0.0029261105228215456\n",
      "Step: 1183, Loss: 0.0019581264350563288\n",
      "Step: 1184, Loss: 0.002499453956261277\n",
      "Step: 1185, Loss: 0.46762943267822266\n",
      "Step: 1186, Loss: 0.00445037754252553\n",
      "Step: 1187, Loss: 0.002672563772648573\n",
      "Step: 1188, Loss: 0.002379724755883217\n",
      "Step: 1189, Loss: 0.0015214472077786922\n",
      "Step: 1190, Loss: 0.0019672317430377007\n",
      "Step: 1191, Loss: 0.02159889228641987\n",
      "Step: 1192, Loss: 0.0033195498399436474\n",
      "Step: 1193, Loss: 0.0033907494507730007\n",
      "Step: 1194, Loss: 0.0012398414546623826\n",
      "Step: 1195, Loss: 0.0032437860500067472\n",
      "Step: 1196, Loss: 0.002710177330300212\n",
      "Step: 1197, Loss: 0.0018799357349053025\n",
      "Step: 1198, Loss: 0.0018638515612110496\n",
      "Step: 1199, Loss: 0.0016929726116359234\n",
      "Step: 1200, Loss: 0.0006738733500242233\n"
     ]
    }
   ],
   "source": [
    "diffy.train()\n",
    "#print('Estimated time of completion: {}'.format())\n",
    "for it in range(ranges):\n",
    "    optimizer.zero_grad()\n",
    "    img, true_out = next(trainset)\n",
    "    img = T.squeeze(img, 1)\n",
    "\n",
    "    output, (controller_hidden, memory, read_vectors) = diffy(img, (None, memory, None), reset_experience=False)\n",
    "\n",
    "    newt = T.sum(output, (1, 2), keepdim=True)\t\n",
    "\n",
    "    loss = loss_fn(newt, true_out.to(T.float).reshape((50,1,1)))\n",
    "    \n",
    "    loss.backward()\n",
    "    #T.nn.utils.clip_grad_norm_(diffy.parameters(), 2)\n",
    "    optimizer.step()\n",
    "\n",
    "    memory = { k : (v.detach() if isinstance(v, T.autograd.Variable) else v) for k, v in memory.items()}\n",
    "\n",
    "    print('Step: {}, Loss: {}'.format(it+1, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saved at 30 ep acc 99.62333333333333%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader = T.utils.data.DataLoader(dataset=test, batch_size=50, shuffle=True)\n",
    "testset = iter(test_data_loader)\n",
    "testsize = len(test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1, Accuracy: 1.0\n",
      "Step: 2, Accuracy: 1.0\n",
      "Step: 3, Accuracy: 1.0\n",
      "Step: 4, Accuracy: 0.995\n",
      "Step: 5, Accuracy: 0.996\n",
      "Step: 6, Accuracy: 0.9966666666666667\n",
      "Step: 7, Accuracy: 0.9971428571428571\n",
      "Step: 8, Accuracy: 0.9925\n",
      "Step: 9, Accuracy: 0.9933333333333333\n",
      "Step: 10, Accuracy: 0.994\n",
      "Step: 11, Accuracy: 0.9945454545454545\n",
      "Step: 12, Accuracy: 0.995\n",
      "Step: 13, Accuracy: 0.9953846153846154\n",
      "Step: 14, Accuracy: 0.9957142857142857\n",
      "Step: 15, Accuracy: 0.996\n",
      "Step: 16, Accuracy: 0.995\n",
      "Step: 17, Accuracy: 0.9952941176470588\n",
      "Step: 18, Accuracy: 0.9955555555555555\n",
      "Step: 19, Accuracy: 0.9957894736842106\n",
      "Step: 20, Accuracy: 0.996\n",
      "Step: 21, Accuracy: 0.9952380952380953\n",
      "Step: 22, Accuracy: 0.9954545454545455\n",
      "Step: 23, Accuracy: 0.9956521739130435\n",
      "Step: 24, Accuracy: 0.9958333333333333\n",
      "Step: 25, Accuracy: 0.9952\n",
      "Step: 26, Accuracy: 0.9953846153846154\n",
      "Step: 27, Accuracy: 0.9955555555555555\n",
      "Step: 28, Accuracy: 0.9957142857142857\n",
      "Step: 29, Accuracy: 0.9958620689655172\n",
      "Step: 30, Accuracy: 0.996\n",
      "Step: 31, Accuracy: 0.9961290322580645\n",
      "Step: 32, Accuracy: 0.995\n",
      "Step: 33, Accuracy: 0.9945454545454545\n",
      "Step: 34, Accuracy: 0.9947058823529412\n",
      "Step: 35, Accuracy: 0.9937142857142857\n",
      "Step: 36, Accuracy: 0.9938888888888889\n",
      "Step: 37, Accuracy: 0.9935135135135135\n",
      "Step: 38, Accuracy: 0.9936842105263158\n",
      "Step: 39, Accuracy: 0.9938461538461538\n",
      "Step: 40, Accuracy: 0.994\n",
      "Step: 41, Accuracy: 0.9941463414634146\n",
      "Step: 42, Accuracy: 0.9942857142857143\n",
      "Step: 43, Accuracy: 0.9944186046511628\n",
      "Step: 44, Accuracy: 0.9945454545454545\n",
      "Step: 45, Accuracy: 0.9946666666666667\n",
      "Step: 46, Accuracy: 0.9947826086956522\n",
      "Step: 47, Accuracy: 0.9948936170212765\n",
      "Step: 48, Accuracy: 0.995\n",
      "Step: 49, Accuracy: 0.9946938775510205\n",
      "Step: 50, Accuracy: 0.9948\n",
      "Step: 51, Accuracy: 0.9949019607843137\n",
      "Step: 52, Accuracy: 0.995\n",
      "Step: 53, Accuracy: 0.9950943396226415\n",
      "Step: 54, Accuracy: 0.9951851851851852\n",
      "Step: 55, Accuracy: 0.9952727272727273\n",
      "Step: 56, Accuracy: 0.9953571428571428\n",
      "Step: 57, Accuracy: 0.9954385964912281\n",
      "Step: 58, Accuracy: 0.9955172413793103\n",
      "Step: 59, Accuracy: 0.995593220338983\n",
      "Step: 60, Accuracy: 0.9956666666666667\n",
      "Step: 61, Accuracy: 0.9957377049180328\n",
      "Step: 62, Accuracy: 0.9958064516129033\n",
      "Step: 63, Accuracy: 0.9955555555555555\n",
      "Step: 64, Accuracy: 0.995625\n",
      "Step: 65, Accuracy: 0.9956923076923077\n",
      "Step: 66, Accuracy: 0.9957575757575757\n",
      "Step: 67, Accuracy: 0.9955223880597015\n",
      "Step: 68, Accuracy: 0.9955882352941177\n",
      "Step: 69, Accuracy: 0.9956521739130435\n",
      "Step: 70, Accuracy: 0.9954285714285714\n",
      "Step: 71, Accuracy: 0.9954929577464788\n",
      "Step: 72, Accuracy: 0.995\n",
      "Step: 73, Accuracy: 0.9950684931506849\n",
      "Step: 74, Accuracy: 0.9951351351351352\n",
      "Step: 75, Accuracy: 0.9952\n",
      "Step: 76, Accuracy: 0.9952631578947368\n",
      "Step: 77, Accuracy: 0.9953246753246753\n",
      "Step: 78, Accuracy: 0.9953846153846154\n",
      "Step: 79, Accuracy: 0.9954430379746836\n",
      "Step: 80, Accuracy: 0.9955\n",
      "Step: 81, Accuracy: 0.9955555555555555\n",
      "Step: 82, Accuracy: 0.995609756097561\n",
      "Step: 83, Accuracy: 0.9956626506024097\n",
      "Step: 84, Accuracy: 0.9957142857142857\n",
      "Step: 85, Accuracy: 0.9952941176470588\n",
      "Step: 86, Accuracy: 0.9953488372093023\n",
      "Step: 87, Accuracy: 0.9954022988505747\n",
      "Step: 88, Accuracy: 0.9954545454545455\n",
      "Step: 89, Accuracy: 0.9955056179775281\n",
      "Step: 90, Accuracy: 0.9953333333333333\n",
      "Step: 91, Accuracy: 0.9953846153846154\n",
      "Step: 92, Accuracy: 0.9954347826086957\n",
      "Step: 93, Accuracy: 0.9954838709677419\n",
      "Step: 94, Accuracy: 0.995531914893617\n",
      "Step: 95, Accuracy: 0.995578947368421\n",
      "Step: 96, Accuracy: 0.995625\n",
      "Step: 97, Accuracy: 0.9956701030927835\n",
      "Step: 98, Accuracy: 0.9955102040816326\n",
      "Step: 99, Accuracy: 0.9955555555555555\n",
      "Step: 100, Accuracy: 0.9956\n",
      "Step: 101, Accuracy: 0.9956435643564356\n",
      "Step: 102, Accuracy: 0.995686274509804\n",
      "Step: 103, Accuracy: 0.9957281553398059\n",
      "Step: 104, Accuracy: 0.9957692307692307\n",
      "Step: 105, Accuracy: 0.9958095238095238\n",
      "Step: 106, Accuracy: 0.9956603773584906\n",
      "Step: 107, Accuracy: 0.9955140186915887\n",
      "Step: 108, Accuracy: 0.9955555555555555\n",
      "Step: 109, Accuracy: 0.9955963302752293\n",
      "Step: 110, Accuracy: 0.9956363636363637\n",
      "Step: 111, Accuracy: 0.9956756756756757\n",
      "Step: 112, Accuracy: 0.9955357142857143\n",
      "Step: 113, Accuracy: 0.995575221238938\n",
      "Step: 114, Accuracy: 0.9956140350877193\n",
      "Step: 115, Accuracy: 0.9954782608695653\n",
      "Step: 116, Accuracy: 0.9955172413793103\n",
      "Step: 117, Accuracy: 0.9955555555555555\n",
      "Step: 118, Accuracy: 0.995593220338983\n",
      "Step: 119, Accuracy: 0.9956302521008403\n",
      "Step: 120, Accuracy: 0.9956666666666667\n",
      "Step: 121, Accuracy: 0.9957024793388429\n",
      "Step: 122, Accuracy: 0.9957377049180328\n",
      "Step: 123, Accuracy: 0.995609756097561\n",
      "Step: 124, Accuracy: 0.9956451612903225\n",
      "Step: 125, Accuracy: 0.99568\n",
      "Step: 126, Accuracy: 0.9957142857142857\n",
      "Step: 127, Accuracy: 0.995748031496063\n",
      "Step: 128, Accuracy: 0.995625\n",
      "Step: 129, Accuracy: 0.9956589147286822\n",
      "Step: 130, Accuracy: 0.9956923076923077\n",
      "Step: 131, Accuracy: 0.9955725190839695\n",
      "Step: 132, Accuracy: 0.9956060606060606\n",
      "Step: 133, Accuracy: 0.9956390977443609\n",
      "Step: 134, Accuracy: 0.9956716417910447\n",
      "Step: 135, Accuracy: 0.9957037037037038\n",
      "Step: 136, Accuracy: 0.995735294117647\n",
      "Step: 137, Accuracy: 0.9957664233576642\n",
      "Step: 138, Accuracy: 0.9957971014492754\n",
      "Step: 139, Accuracy: 0.9958273381294964\n",
      "Step: 140, Accuracy: 0.9958571428571429\n",
      "Step: 141, Accuracy: 0.995886524822695\n",
      "Step: 142, Accuracy: 0.9959154929577465\n",
      "Step: 143, Accuracy: 0.995944055944056\n",
      "Step: 144, Accuracy: 0.9959722222222223\n",
      "Step: 145, Accuracy: 0.996\n",
      "Step: 146, Accuracy: 0.996027397260274\n",
      "Step: 147, Accuracy: 0.9960544217687075\n",
      "Step: 148, Accuracy: 0.9960810810810811\n",
      "Step: 149, Accuracy: 0.9961073825503356\n",
      "Step: 150, Accuracy: 0.996\n",
      "Step: 151, Accuracy: 0.9957615894039735\n",
      "Step: 152, Accuracy: 0.9957894736842106\n",
      "Step: 153, Accuracy: 0.9958169934640523\n",
      "Step: 154, Accuracy: 0.9958441558441559\n",
      "Step: 155, Accuracy: 0.995741935483871\n",
      "Step: 156, Accuracy: 0.9957692307692307\n",
      "Step: 157, Accuracy: 0.9957961783439491\n",
      "Step: 158, Accuracy: 0.9958227848101265\n",
      "Step: 159, Accuracy: 0.9958490566037735\n",
      "Step: 160, Accuracy: 0.99575\n",
      "Step: 161, Accuracy: 0.9957763975155279\n",
      "Step: 162, Accuracy: 0.995679012345679\n",
      "Step: 163, Accuracy: 0.9955828220858896\n",
      "Step: 164, Accuracy: 0.995609756097561\n",
      "Step: 165, Accuracy: 0.9956363636363637\n",
      "Step: 166, Accuracy: 0.9956626506024097\n",
      "Step: 167, Accuracy: 0.995688622754491\n",
      "Step: 168, Accuracy: 0.9955952380952381\n",
      "Step: 169, Accuracy: 0.995621301775148\n",
      "Step: 170, Accuracy: 0.9956470588235294\n",
      "Step: 171, Accuracy: 0.995672514619883\n",
      "Step: 172, Accuracy: 0.9955813953488372\n",
      "Step: 173, Accuracy: 0.9956069364161849\n",
      "Step: 174, Accuracy: 0.995632183908046\n",
      "Step: 175, Accuracy: 0.9956571428571429\n",
      "Step: 176, Accuracy: 0.9955681818181819\n",
      "Step: 177, Accuracy: 0.995593220338983\n",
      "Step: 178, Accuracy: 0.9956179775280899\n",
      "Step: 179, Accuracy: 0.9955307262569832\n",
      "Step: 180, Accuracy: 0.9955555555555555\n",
      "Step: 181, Accuracy: 0.9955801104972376\n",
      "Step: 182, Accuracy: 0.9956043956043956\n",
      "Step: 183, Accuracy: 0.9956284153005465\n",
      "Step: 184, Accuracy: 0.9956521739130435\n",
      "Step: 185, Accuracy: 0.9956756756756757\n",
      "Step: 186, Accuracy: 0.9956989247311828\n",
      "Step: 187, Accuracy: 0.9955080213903743\n",
      "Step: 188, Accuracy: 0.995531914893617\n",
      "Step: 189, Accuracy: 0.9955555555555555\n",
      "Step: 190, Accuracy: 0.995578947368421\n",
      "Step: 191, Accuracy: 0.9956020942408377\n",
      "Step: 192, Accuracy: 0.995625\n",
      "Step: 193, Accuracy: 0.9956476683937824\n",
      "Step: 194, Accuracy: 0.9956701030927835\n",
      "Step: 195, Accuracy: 0.9956923076923077\n",
      "Step: 196, Accuracy: 0.9957142857142857\n",
      "Step: 197, Accuracy: 0.995736040609137\n",
      "Step: 198, Accuracy: 0.9956565656565657\n",
      "Step: 199, Accuracy: 0.995678391959799\n",
      "Step: 200, Accuracy: 0.9957\n",
      "Step: 201, Accuracy: 0.9957213930348259\n",
      "Step: 202, Accuracy: 0.9957425742574257\n",
      "Step: 203, Accuracy: 0.9957635467980296\n",
      "Step: 204, Accuracy: 0.9957843137254901\n",
      "Step: 205, Accuracy: 0.9958048780487805\n",
      "Step: 206, Accuracy: 0.9958252427184466\n",
      "Step: 207, Accuracy: 0.9957487922705314\n",
      "Step: 208, Accuracy: 0.9957692307692307\n",
      "Step: 209, Accuracy: 0.9957894736842106\n",
      "Step: 210, Accuracy: 0.9958095238095238\n",
      "Step: 211, Accuracy: 0.995829383886256\n",
      "Step: 212, Accuracy: 0.9958490566037735\n",
      "Step: 213, Accuracy: 0.995868544600939\n",
      "Step: 214, Accuracy: 0.9958878504672897\n",
      "Step: 215, Accuracy: 0.995906976744186\n",
      "Step: 216, Accuracy: 0.9959259259259259\n",
      "Step: 217, Accuracy: 0.995852534562212\n",
      "Step: 218, Accuracy: 0.9957798165137615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 219, Accuracy: 0.9957990867579909\n",
      "Step: 220, Accuracy: 0.9958181818181818\n",
      "Step: 221, Accuracy: 0.9958371040723982\n",
      "Step: 222, Accuracy: 0.9958558558558559\n",
      "Step: 223, Accuracy: 0.9957847533632287\n",
      "Step: 224, Accuracy: 0.9958035714285715\n",
      "Step: 225, Accuracy: 0.9958222222222223\n",
      "Step: 226, Accuracy: 0.9958407079646018\n",
      "Step: 227, Accuracy: 0.9956828193832599\n",
      "Step: 228, Accuracy: 0.9957017543859649\n",
      "Step: 229, Accuracy: 0.9957205240174672\n",
      "Step: 230, Accuracy: 0.9957391304347826\n",
      "Step: 231, Accuracy: 0.9957575757575757\n",
      "Step: 232, Accuracy: 0.9957758620689655\n",
      "Step: 233, Accuracy: 0.995793991416309\n",
      "Step: 234, Accuracy: 0.9957264957264957\n",
      "Step: 235, Accuracy: 0.9957446808510638\n",
      "Step: 236, Accuracy: 0.9957627118644068\n",
      "Step: 237, Accuracy: 0.9957805907172996\n",
      "Step: 238, Accuracy: 0.9957983193277311\n",
      "Step: 239, Accuracy: 0.99581589958159\n",
      "Step: 240, Accuracy: 0.9958333333333333\n",
      "Step: 241, Accuracy: 0.995850622406639\n",
      "Step: 242, Accuracy: 0.9958677685950413\n",
      "Step: 243, Accuracy: 0.9958847736625515\n",
      "Step: 244, Accuracy: 0.9959016393442623\n",
      "Step: 245, Accuracy: 0.9959183673469387\n",
      "Step: 246, Accuracy: 0.9959349593495935\n",
      "Step: 247, Accuracy: 0.9959514170040485\n",
      "Step: 248, Accuracy: 0.9959677419354839\n",
      "Step: 249, Accuracy: 0.9959839357429718\n",
      "Step: 250, Accuracy: 0.996\n",
      "Step: 251, Accuracy: 0.9959362549800796\n",
      "Step: 252, Accuracy: 0.9959523809523809\n",
      "Step: 253, Accuracy: 0.9959683794466403\n",
      "Step: 254, Accuracy: 0.9959842519685039\n",
      "Step: 255, Accuracy: 0.996\n",
      "Step: 256, Accuracy: 0.996015625\n",
      "Step: 257, Accuracy: 0.9957976653696498\n",
      "Step: 258, Accuracy: 0.9958139534883721\n",
      "Step: 259, Accuracy: 0.9958301158301158\n",
      "Step: 260, Accuracy: 0.9957692307692307\n",
      "Step: 261, Accuracy: 0.9957854406130269\n",
      "Step: 262, Accuracy: 0.9958015267175573\n",
      "Step: 263, Accuracy: 0.9957414448669202\n",
      "Step: 264, Accuracy: 0.9957575757575757\n",
      "Step: 265, Accuracy: 0.9956981132075472\n",
      "Step: 266, Accuracy: 0.9957142857142857\n",
      "Step: 267, Accuracy: 0.9957303370786517\n",
      "Step: 268, Accuracy: 0.9956716417910447\n",
      "Step: 269, Accuracy: 0.9956877323420075\n",
      "Step: 270, Accuracy: 0.9956296296296296\n",
      "Step: 271, Accuracy: 0.9956457564575646\n",
      "Step: 272, Accuracy: 0.9956617647058823\n",
      "Step: 273, Accuracy: 0.9956776556776556\n",
      "Step: 274, Accuracy: 0.9956934306569343\n",
      "Step: 275, Accuracy: 0.9957090909090909\n",
      "Step: 276, Accuracy: 0.9957246376811594\n",
      "Step: 277, Accuracy: 0.9957400722021661\n",
      "Step: 278, Accuracy: 0.9957553956834533\n",
      "Step: 279, Accuracy: 0.9957706093189964\n",
      "Step: 280, Accuracy: 0.9957857142857143\n",
      "Step: 281, Accuracy: 0.9958007117437723\n",
      "Step: 282, Accuracy: 0.9958156028368794\n",
      "Step: 283, Accuracy: 0.9958303886925796\n",
      "Step: 284, Accuracy: 0.9958450704225352\n",
      "Step: 285, Accuracy: 0.995859649122807\n",
      "Step: 286, Accuracy: 0.9958741258741258\n",
      "Step: 287, Accuracy: 0.9958885017421603\n",
      "Step: 288, Accuracy: 0.9959027777777778\n",
      "Step: 289, Accuracy: 0.995916955017301\n",
      "Step: 290, Accuracy: 0.9959310344827587\n",
      "Step: 291, Accuracy: 0.9959450171821306\n",
      "Step: 292, Accuracy: 0.9959589041095891\n",
      "Step: 293, Accuracy: 0.9959726962457338\n",
      "Step: 294, Accuracy: 0.9959863945578231\n",
      "Step: 295, Accuracy: 0.996\n",
      "Step: 296, Accuracy: 0.9960135135135135\n",
      "Step: 297, Accuracy: 0.996026936026936\n",
      "Step: 298, Accuracy: 0.9960402684563758\n",
      "Step: 299, Accuracy: 0.9960535117056856\n",
      "Step: 300, Accuracy: 0.9960666666666667\n",
      "Step: 301, Accuracy: 0.9960132890365448\n",
      "Step: 302, Accuracy: 0.9960264900662251\n",
      "Step: 303, Accuracy: 0.996039603960396\n",
      "Step: 304, Accuracy: 0.9960526315789474\n",
      "Step: 305, Accuracy: 0.9960655737704918\n",
      "Step: 306, Accuracy: 0.996078431372549\n",
      "Step: 307, Accuracy: 0.9960912052117263\n",
      "Step: 308, Accuracy: 0.9961038961038962\n",
      "Step: 309, Accuracy: 0.996116504854369\n",
      "Step: 310, Accuracy: 0.9960645161290322\n",
      "Step: 311, Accuracy: 0.9960771704180065\n",
      "Step: 312, Accuracy: 0.9960256410256411\n",
      "Step: 313, Accuracy: 0.996038338658147\n",
      "Step: 314, Accuracy: 0.9960509554140128\n",
      "Step: 315, Accuracy: 0.9960634920634921\n",
      "Step: 316, Accuracy: 0.9960759493670887\n",
      "Step: 317, Accuracy: 0.9960883280757098\n",
      "Step: 318, Accuracy: 0.9961006289308176\n",
      "Step: 319, Accuracy: 0.9961128526645768\n",
      "Step: 320, Accuracy: 0.996125\n",
      "Step: 321, Accuracy: 0.9961370716510903\n",
      "Step: 322, Accuracy: 0.9961490683229813\n",
      "Step: 323, Accuracy: 0.9961609907120743\n",
      "Step: 324, Accuracy: 0.9961728395061729\n",
      "Step: 325, Accuracy: 0.9961846153846153\n",
      "Step: 326, Accuracy: 0.9961963190184049\n",
      "Step: 327, Accuracy: 0.9962079510703364\n",
      "Step: 328, Accuracy: 0.996219512195122\n",
      "Step: 329, Accuracy: 0.9961702127659574\n",
      "Step: 330, Accuracy: 0.9961818181818182\n",
      "Step: 331, Accuracy: 0.9961329305135952\n",
      "Step: 332, Accuracy: 0.996144578313253\n",
      "Step: 333, Accuracy: 0.9961561561561562\n",
      "Step: 334, Accuracy: 0.9961676646706586\n",
      "Step: 335, Accuracy: 0.9961194029850746\n",
      "Step: 336, Accuracy: 0.9961309523809524\n",
      "Step: 337, Accuracy: 0.9961424332344213\n",
      "Step: 338, Accuracy: 0.9961538461538462\n",
      "Step: 339, Accuracy: 0.996165191740413\n",
      "Step: 340, Accuracy: 0.9961764705882353\n",
      "Step: 341, Accuracy: 0.9961876832844575\n",
      "Step: 342, Accuracy: 0.9961988304093568\n",
      "Step: 343, Accuracy: 0.9961516034985423\n",
      "Step: 344, Accuracy: 0.9961627906976744\n",
      "Step: 345, Accuracy: 0.9961739130434782\n",
      "Step: 346, Accuracy: 0.9961849710982659\n",
      "Step: 347, Accuracy: 0.9961383285302594\n",
      "Step: 348, Accuracy: 0.9961494252873563\n",
      "Step: 349, Accuracy: 0.996160458452722\n",
      "Step: 350, Accuracy: 0.9961714285714286\n",
      "Step: 351, Accuracy: 0.9961823361823362\n",
      "Step: 352, Accuracy: 0.9961931818181818\n",
      "Step: 353, Accuracy: 0.9962039660056657\n",
      "Step: 354, Accuracy: 0.9962146892655367\n",
      "Step: 355, Accuracy: 0.996225352112676\n",
      "Step: 356, Accuracy: 0.9961797752808988\n",
      "Step: 357, Accuracy: 0.9961904761904762\n",
      "Step: 358, Accuracy: 0.9962011173184357\n",
      "Step: 359, Accuracy: 0.9962116991643454\n",
      "Step: 360, Accuracy: 0.9962222222222222\n",
      "Step: 361, Accuracy: 0.9961772853185595\n",
      "Step: 362, Accuracy: 0.9961878453038674\n",
      "Step: 363, Accuracy: 0.996198347107438\n",
      "Step: 364, Accuracy: 0.9962087912087912\n",
      "Step: 365, Accuracy: 0.9962191780821917\n",
      "Step: 366, Accuracy: 0.9962295081967213\n",
      "Step: 367, Accuracy: 0.9962397820163488\n",
      "Step: 368, Accuracy: 0.99625\n",
      "Step: 369, Accuracy: 0.996260162601626\n",
      "Step: 370, Accuracy: 0.9962702702702703\n",
      "Step: 371, Accuracy: 0.9962264150943396\n",
      "Step: 372, Accuracy: 0.9962365591397849\n",
      "Step: 373, Accuracy: 0.9962466487935657\n",
      "Step: 374, Accuracy: 0.9962032085561497\n",
      "Step: 375, Accuracy: 0.9962133333333333\n",
      "Step: 376, Accuracy: 0.9962234042553192\n",
      "Step: 377, Accuracy: 0.9961803713527851\n",
      "Step: 378, Accuracy: 0.9961904761904762\n",
      "Step: 379, Accuracy: 0.9961477572559366\n",
      "Step: 380, Accuracy: 0.9961578947368421\n",
      "Step: 381, Accuracy: 0.9961679790026247\n",
      "Step: 382, Accuracy: 0.9961780104712041\n",
      "Step: 383, Accuracy: 0.9961879895561357\n",
      "Step: 384, Accuracy: 0.9961979166666667\n",
      "Step: 385, Accuracy: 0.9962077922077922\n",
      "Step: 386, Accuracy: 0.9961658031088083\n",
      "Step: 387, Accuracy: 0.9961757105943152\n",
      "Step: 388, Accuracy: 0.9961855670103092\n",
      "Step: 389, Accuracy: 0.9961953727506426\n",
      "Step: 390, Accuracy: 0.9962051282051282\n",
      "Step: 391, Accuracy: 0.9962148337595907\n",
      "Step: 392, Accuracy: 0.9961734693877551\n",
      "Step: 393, Accuracy: 0.9961832061068703\n",
      "Step: 394, Accuracy: 0.9961928934010152\n",
      "Step: 395, Accuracy: 0.9962025316455696\n",
      "Step: 396, Accuracy: 0.9962121212121212\n",
      "Step: 397, Accuracy: 0.9962216624685138\n",
      "Step: 398, Accuracy: 0.9962311557788944\n",
      "Step: 399, Accuracy: 0.9962406015037594\n",
      "Step: 400, Accuracy: 0.99625\n",
      "Step: 401, Accuracy: 0.9962593516209476\n",
      "Step: 402, Accuracy: 0.996268656716418\n",
      "Step: 403, Accuracy: 0.9962779156327544\n",
      "Step: 404, Accuracy: 0.9962871287128713\n",
      "Step: 405, Accuracy: 0.9962469135802469\n",
      "Step: 406, Accuracy: 0.9962561576354679\n",
      "Step: 407, Accuracy: 0.9962653562653563\n",
      "Step: 408, Accuracy: 0.9962745098039215\n",
      "Step: 409, Accuracy: 0.9962347188264059\n",
      "Step: 410, Accuracy: 0.9962439024390244\n",
      "Step: 411, Accuracy: 0.9962530413625305\n",
      "Step: 412, Accuracy: 0.9962135922330098\n",
      "Step: 413, Accuracy: 0.9962227602905569\n",
      "Step: 414, Accuracy: 0.996231884057971\n",
      "Step: 415, Accuracy: 0.9962409638554217\n",
      "Step: 416, Accuracy: 0.99625\n",
      "Step: 417, Accuracy: 0.9962589928057554\n",
      "Step: 418, Accuracy: 0.9962679425837321\n",
      "Step: 419, Accuracy: 0.9962768496420048\n",
      "Step: 420, Accuracy: 0.9962857142857143\n",
      "Step: 421, Accuracy: 0.9962945368171021\n",
      "Step: 422, Accuracy: 0.996303317535545\n",
      "Step: 423, Accuracy: 0.9963120567375886\n",
      "Step: 424, Accuracy: 0.9963207547169811\n",
      "Step: 425, Accuracy: 0.9963294117647059\n",
      "Step: 426, Accuracy: 0.9963380281690141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 427, Accuracy: 0.9963466042154566\n",
      "Step: 428, Accuracy: 0.9963084112149533\n",
      "Step: 429, Accuracy: 0.9963170163170163\n",
      "Step: 430, Accuracy: 0.9962790697674418\n",
      "Step: 431, Accuracy: 0.9962877030162413\n",
      "Step: 432, Accuracy: 0.9962962962962963\n",
      "Step: 433, Accuracy: 0.9963048498845266\n",
      "Step: 434, Accuracy: 0.9963133640552996\n",
      "Step: 435, Accuracy: 0.9963218390804598\n",
      "Step: 436, Accuracy: 0.9963302752293578\n",
      "Step: 437, Accuracy: 0.9962929061784898\n",
      "Step: 438, Accuracy: 0.9963013698630137\n",
      "Step: 439, Accuracy: 0.9963097949886105\n",
      "Step: 440, Accuracy: 0.9962727272727273\n",
      "Step: 441, Accuracy: 0.996281179138322\n",
      "Step: 442, Accuracy: 0.996289592760181\n",
      "Step: 443, Accuracy: 0.9962979683972912\n",
      "Step: 444, Accuracy: 0.9963063063063063\n",
      "Step: 445, Accuracy: 0.9962696629213483\n",
      "Step: 446, Accuracy: 0.9962331838565023\n",
      "Step: 447, Accuracy: 0.996241610738255\n",
      "Step: 448, Accuracy: 0.99625\n",
      "Step: 449, Accuracy: 0.9962583518930958\n",
      "Step: 450, Accuracy: 0.9962666666666666\n",
      "Step: 451, Accuracy: 0.9962749445676274\n",
      "Step: 452, Accuracy: 0.996283185840708\n",
      "Step: 453, Accuracy: 0.9962472406181015\n",
      "Step: 454, Accuracy: 0.9962114537444934\n",
      "Step: 455, Accuracy: 0.9962197802197802\n",
      "Step: 456, Accuracy: 0.9962280701754386\n",
      "Step: 457, Accuracy: 0.9962363238512035\n",
      "Step: 458, Accuracy: 0.9962445414847162\n",
      "Step: 459, Accuracy: 0.9962527233115468\n",
      "Step: 460, Accuracy: 0.9962608695652174\n",
      "Step: 461, Accuracy: 0.9962689804772235\n",
      "Step: 462, Accuracy: 0.9962770562770563\n",
      "Step: 463, Accuracy: 0.9962850971922246\n",
      "Step: 464, Accuracy: 0.99625\n",
      "Step: 465, Accuracy: 0.9962580645161291\n",
      "Step: 466, Accuracy: 0.9962660944206009\n",
      "Step: 467, Accuracy: 0.9962740899357602\n",
      "Step: 468, Accuracy: 0.9962820512820513\n",
      "Step: 469, Accuracy: 0.9962899786780384\n",
      "Step: 470, Accuracy: 0.9962553191489362\n",
      "Step: 471, Accuracy: 0.9962632696390659\n",
      "Step: 472, Accuracy: 0.9962711864406779\n",
      "Step: 473, Accuracy: 0.9962790697674418\n",
      "Step: 474, Accuracy: 0.9962869198312236\n",
      "Step: 475, Accuracy: 0.9962947368421052\n",
      "Step: 476, Accuracy: 0.9963025210084033\n",
      "Step: 477, Accuracy: 0.9963102725366876\n",
      "Step: 478, Accuracy: 0.9963179916317991\n",
      "Step: 479, Accuracy: 0.9962839248434238\n",
      "Step: 480, Accuracy: 0.9962916666666667\n",
      "Step: 481, Accuracy: 0.9962993762993763\n",
      "Step: 482, Accuracy: 0.9963070539419087\n",
      "Step: 483, Accuracy: 0.9963146997929606\n",
      "Step: 484, Accuracy: 0.9963223140495868\n",
      "Step: 485, Accuracy: 0.9962886597938144\n",
      "Step: 486, Accuracy: 0.9962962962962963\n",
      "Step: 487, Accuracy: 0.9963039014373717\n",
      "Step: 488, Accuracy: 0.9963114754098361\n",
      "Step: 489, Accuracy: 0.996319018404908\n",
      "Step: 490, Accuracy: 0.9963265306122449\n",
      "Step: 491, Accuracy: 0.9962932790224033\n",
      "Step: 492, Accuracy: 0.9963008130081301\n",
      "Step: 493, Accuracy: 0.9963083164300203\n",
      "Step: 494, Accuracy: 0.9963157894736843\n",
      "Step: 495, Accuracy: 0.9963232323232323\n",
      "Step: 496, Accuracy: 0.9963306451612903\n",
      "Step: 497, Accuracy: 0.9963380281690141\n",
      "Step: 498, Accuracy: 0.9963453815261044\n",
      "Step: 499, Accuracy: 0.996312625250501\n",
      "Step: 500, Accuracy: 0.99632\n",
      "Step: 501, Accuracy: 0.9963273453093813\n",
      "Step: 502, Accuracy: 0.9963346613545817\n",
      "Step: 503, Accuracy: 0.9963021868787276\n",
      "Step: 504, Accuracy: 0.9963095238095238\n",
      "Step: 505, Accuracy: 0.9963168316831683\n",
      "Step: 506, Accuracy: 0.9963241106719367\n",
      "Step: 507, Accuracy: 0.9963313609467456\n",
      "Step: 508, Accuracy: 0.9963385826771654\n",
      "Step: 509, Accuracy: 0.9963457760314341\n",
      "Step: 510, Accuracy: 0.9963529411764706\n",
      "Step: 511, Accuracy: 0.9963600782778865\n",
      "Step: 512, Accuracy: 0.9963671875\n",
      "Step: 513, Accuracy: 0.9963742690058479\n",
      "Step: 514, Accuracy: 0.9963813229571984\n",
      "Step: 515, Accuracy: 0.9963883495145631\n",
      "Step: 516, Accuracy: 0.9963953488372093\n",
      "Step: 517, Accuracy: 0.9964023210831722\n",
      "Step: 518, Accuracy: 0.9964092664092664\n",
      "Step: 519, Accuracy: 0.9964161849710983\n",
      "Step: 520, Accuracy: 0.9963846153846154\n",
      "Step: 521, Accuracy: 0.9963915547024952\n",
      "Step: 522, Accuracy: 0.996360153256705\n",
      "Step: 523, Accuracy: 0.9963288718929254\n",
      "Step: 524, Accuracy: 0.9963358778625954\n",
      "Step: 525, Accuracy: 0.9963428571428572\n",
      "Step: 526, Accuracy: 0.9963498098859316\n",
      "Step: 527, Accuracy: 0.9963567362428842\n",
      "Step: 528, Accuracy: 0.9963636363636363\n",
      "Step: 529, Accuracy: 0.9963705103969754\n",
      "Step: 530, Accuracy: 0.9963773584905661\n",
      "Step: 531, Accuracy: 0.9963841807909605\n",
      "Step: 532, Accuracy: 0.996390977443609\n",
      "Step: 533, Accuracy: 0.9963977485928706\n",
      "Step: 534, Accuracy: 0.9964044943820225\n",
      "Step: 535, Accuracy: 0.996411214953271\n",
      "Step: 536, Accuracy: 0.9964179104477612\n",
      "Step: 537, Accuracy: 0.9964245810055866\n",
      "Step: 538, Accuracy: 0.9964312267657992\n",
      "Step: 539, Accuracy: 0.9964378478664193\n",
      "Step: 540, Accuracy: 0.9964444444444445\n",
      "Step: 541, Accuracy: 0.9964510166358596\n",
      "Step: 542, Accuracy: 0.9964575645756457\n",
      "Step: 543, Accuracy: 0.996427255985267\n",
      "Step: 544, Accuracy: 0.9964338235294118\n",
      "Step: 545, Accuracy: 0.9964403669724771\n",
      "Step: 546, Accuracy: 0.9964468864468864\n",
      "Step: 547, Accuracy: 0.9964533820840951\n",
      "Step: 548, Accuracy: 0.9964598540145986\n",
      "Step: 549, Accuracy: 0.9964663023679418\n",
      "Step: 550, Accuracy: 0.9964727272727273\n",
      "Step: 551, Accuracy: 0.9964791288566244\n",
      "Step: 552, Accuracy: 0.9964492753623189\n",
      "Step: 553, Accuracy: 0.9964556962025316\n",
      "Step: 554, Accuracy: 0.9964620938628159\n",
      "Step: 555, Accuracy: 0.9964684684684685\n",
      "Step: 556, Accuracy: 0.9964748201438849\n",
      "Step: 557, Accuracy: 0.9964811490125673\n",
      "Step: 558, Accuracy: 0.9964516129032258\n",
      "Step: 559, Accuracy: 0.9964579606440072\n",
      "Step: 560, Accuracy: 0.9964642857142857\n",
      "Step: 561, Accuracy: 0.9964705882352941\n",
      "Step: 562, Accuracy: 0.99644128113879\n",
      "Step: 563, Accuracy: 0.9964476021314387\n",
      "Step: 564, Accuracy: 0.9964539007092199\n",
      "Step: 565, Accuracy: 0.9964601769911504\n",
      "Step: 566, Accuracy: 0.9964664310954063\n",
      "Step: 567, Accuracy: 0.9964726631393298\n",
      "Step: 568, Accuracy: 0.9964788732394366\n",
      "Step: 569, Accuracy: 0.9964850615114236\n",
      "Step: 570, Accuracy: 0.9964912280701754\n",
      "Step: 571, Accuracy: 0.9964973730297724\n",
      "Step: 572, Accuracy: 0.9965034965034965\n",
      "Step: 573, Accuracy: 0.9965095986038395\n",
      "Step: 574, Accuracy: 0.9965156794425087\n",
      "Step: 575, Accuracy: 0.9964869565217391\n",
      "Step: 576, Accuracy: 0.9964930555555556\n",
      "Step: 577, Accuracy: 0.9964991334488735\n",
      "Step: 578, Accuracy: 0.9965051903114187\n",
      "Step: 579, Accuracy: 0.9965112262521589\n",
      "Step: 580, Accuracy: 0.9965172413793103\n",
      "Step: 581, Accuracy: 0.9965232358003442\n",
      "Step: 582, Accuracy: 0.9965292096219931\n",
      "Step: 583, Accuracy: 0.9965351629502572\n",
      "Step: 584, Accuracy: 0.996541095890411\n",
      "Step: 585, Accuracy: 0.9965470085470085\n",
      "Step: 586, Accuracy: 0.996518771331058\n",
      "Step: 587, Accuracy: 0.9965247018739353\n",
      "Step: 588, Accuracy: 0.9965306122448979\n",
      "Step: 589, Accuracy: 0.9965365025466894\n",
      "Step: 590, Accuracy: 0.9965423728813559\n",
      "Step: 591, Accuracy: 0.9965482233502538\n",
      "Step: 592, Accuracy: 0.9965540540540541\n",
      "Step: 593, Accuracy: 0.9965598650927487\n",
      "Step: 594, Accuracy: 0.9965656565656565\n",
      "Step: 595, Accuracy: 0.9965714285714286\n",
      "Step: 596, Accuracy: 0.9965771812080537\n",
      "Step: 597, Accuracy: 0.9965829145728643\n",
      "Step: 598, Accuracy: 0.9965886287625418\n",
      "Step: 599, Accuracy: 0.9965943238731219\n",
      "Step: 600, Accuracy: 0.9966\n",
      "Step: 601, Accuracy: 0.9965723793677205\n",
      "Step: 602, Accuracy: 0.996578073089701\n",
      "Step: 603, Accuracy: 0.9965837479270315\n",
      "Step: 604, Accuracy: 0.9965894039735099\n",
      "Step: 605, Accuracy: 0.996595041322314\n",
      "Step: 606, Accuracy: 0.9966006600660066\n",
      "Step: 607, Accuracy: 0.9966062602965403\n",
      "Step: 608, Accuracy: 0.9966118421052632\n",
      "Step: 609, Accuracy: 0.9966174055829228\n",
      "Step: 610, Accuracy: 0.9966229508196721\n",
      "Step: 611, Accuracy: 0.9965957446808511\n",
      "Step: 612, Accuracy: 0.9966013071895424\n",
      "Step: 613, Accuracy: 0.9966068515497553\n",
      "Step: 614, Accuracy: 0.9966123778501629\n",
      "Step: 615, Accuracy: 0.9966178861788618\n",
      "Step: 616, Accuracy: 0.9966233766233766\n",
      "Step: 617, Accuracy: 0.9966288492706645\n",
      "Step: 618, Accuracy: 0.9966343042071197\n",
      "Step: 619, Accuracy: 0.9966397415185784\n",
      "Step: 620, Accuracy: 0.9966129032258064\n",
      "Step: 621, Accuracy: 0.9966183574879227\n",
      "Step: 622, Accuracy: 0.9966237942122187\n",
      "Step: 623, Accuracy: 0.996629213483146\n",
      "Step: 624, Accuracy: 0.9966346153846154\n",
      "Step: 625, Accuracy: 0.99664\n",
      "Step: 626, Accuracy: 0.9966453674121406\n",
      "Step: 627, Accuracy: 0.9966507177033492\n",
      "Step: 628, Accuracy: 0.9966242038216561\n",
      "Step: 629, Accuracy: 0.9966295707472178\n",
      "Step: 630, Accuracy: 0.9966349206349207\n",
      "Step: 631, Accuracy: 0.9966402535657686\n",
      "Step: 632, Accuracy: 0.9966455696202532\n",
      "Step: 633, Accuracy: 0.9966508688783571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 634, Accuracy: 0.9966561514195583\n",
      "Step: 635, Accuracy: 0.9966299212598425\n",
      "Step: 636, Accuracy: 0.9966352201257862\n",
      "Step: 637, Accuracy: 0.9966405023547881\n",
      "Step: 638, Accuracy: 0.9966457680250783\n",
      "Step: 639, Accuracy: 0.9966510172143975\n",
      "Step: 640, Accuracy: 0.99665625\n",
      "Step: 641, Accuracy: 0.9966302652106084\n",
      "Step: 642, Accuracy: 0.9966355140186915\n",
      "Step: 643, Accuracy: 0.9966407465007776\n",
      "Step: 644, Accuracy: 0.9966459627329193\n",
      "Step: 645, Accuracy: 0.9966511627906977\n",
      "Step: 646, Accuracy: 0.996656346749226\n",
      "Step: 647, Accuracy: 0.996661514683153\n",
      "Step: 648, Accuracy: 0.9966666666666667\n",
      "Step: 649, Accuracy: 0.9966718027734977\n",
      "Step: 650, Accuracy: 0.9966461538461538\n",
      "Step: 651, Accuracy: 0.9966513056835637\n",
      "Step: 652, Accuracy: 0.9966564417177914\n",
      "Step: 653, Accuracy: 0.9966309341500765\n",
      "Step: 654, Accuracy: 0.9966360856269113\n",
      "Step: 655, Accuracy: 0.9966412213740458\n",
      "Step: 656, Accuracy: 0.9966463414634147\n",
      "Step: 657, Accuracy: 0.9966514459665144\n",
      "Step: 658, Accuracy: 0.9966565349544073\n",
      "Step: 659, Accuracy: 0.9966616084977238\n",
      "Step: 660, Accuracy: 0.9966666666666667\n",
      "Step: 661, Accuracy: 0.9966717095310136\n",
      "Step: 662, Accuracy: 0.9966767371601208\n",
      "Step: 663, Accuracy: 0.9966817496229261\n",
      "Step: 664, Accuracy: 0.9966867469879518\n",
      "Step: 665, Accuracy: 0.9966917293233083\n",
      "Step: 666, Accuracy: 0.9966966966966967\n",
      "Step: 667, Accuracy: 0.9967016491754123\n",
      "Step: 668, Accuracy: 0.9966766467065868\n",
      "Step: 669, Accuracy: 0.9966517189835575\n",
      "Step: 670, Accuracy: 0.9966567164179104\n",
      "Step: 671, Accuracy: 0.9966616989567809\n",
      "Step: 672, Accuracy: 0.9966666666666667\n",
      "Step: 673, Accuracy: 0.9966419019316494\n",
      "Step: 674, Accuracy: 0.9966468842729971\n",
      "Step: 675, Accuracy: 0.9966518518518519\n",
      "Step: 676, Accuracy: 0.9966272189349112\n",
      "Step: 677, Accuracy: 0.9966322008862629\n",
      "Step: 678, Accuracy: 0.9966371681415929\n",
      "Step: 679, Accuracy: 0.9966126656848306\n",
      "Step: 680, Accuracy: 0.9966176470588235\n",
      "Step: 681, Accuracy: 0.9966226138032306\n",
      "Step: 682, Accuracy: 0.9966275659824047\n",
      "Step: 683, Accuracy: 0.9966032210834553\n",
      "Step: 684, Accuracy: 0.996578947368421\n",
      "Step: 685, Accuracy: 0.9965547445255475\n",
      "Step: 686, Accuracy: 0.9965597667638484\n",
      "Step: 687, Accuracy: 0.9965647743813683\n",
      "Step: 688, Accuracy: 0.9965697674418604\n",
      "Step: 689, Accuracy: 0.9965747460087083\n",
      "Step: 690, Accuracy: 0.9965797101449275\n",
      "Step: 691, Accuracy: 0.9965846599131694\n",
      "Step: 692, Accuracy: 0.9965895953757226\n",
      "Step: 693, Accuracy: 0.9965945165945166\n",
      "Step: 694, Accuracy: 0.996599423631124\n",
      "Step: 695, Accuracy: 0.9966043165467626\n",
      "Step: 696, Accuracy: 0.9966091954022989\n",
      "Step: 697, Accuracy: 0.9966140602582496\n",
      "Step: 698, Accuracy: 0.9966189111747851\n",
      "Step: 699, Accuracy: 0.996623748211731\n",
      "Step: 700, Accuracy: 0.9966285714285714\n",
      "Step: 701, Accuracy: 0.9966333808844507\n",
      "Step: 702, Accuracy: 0.9966381766381767\n",
      "Step: 703, Accuracy: 0.996642958748222\n",
      "Step: 704, Accuracy: 0.9966477272727273\n",
      "Step: 705, Accuracy: 0.9966524822695035\n",
      "Step: 706, Accuracy: 0.996628895184136\n",
      "Step: 707, Accuracy: 0.9966336633663366\n",
      "Step: 708, Accuracy: 0.996638418079096\n",
      "Step: 709, Accuracy: 0.9966431593794076\n",
      "Step: 710, Accuracy: 0.9966478873239437\n",
      "Step: 711, Accuracy: 0.9966526019690577\n",
      "Step: 712, Accuracy: 0.9966573033707865\n",
      "Step: 713, Accuracy: 0.9966619915848527\n",
      "Step: 714, Accuracy: 0.9966666666666667\n",
      "Step: 715, Accuracy: 0.9966713286713287\n",
      "Step: 716, Accuracy: 0.9966759776536313\n",
      "Step: 717, Accuracy: 0.9966806136680614\n",
      "Step: 718, Accuracy: 0.9966573816155989\n",
      "Step: 719, Accuracy: 0.9966620305980528\n",
      "Step: 720, Accuracy: 0.9966666666666667\n",
      "Step: 721, Accuracy: 0.9966435506241331\n",
      "Step: 722, Accuracy: 0.9966204986149585\n",
      "Step: 723, Accuracy: 0.996625172890733\n",
      "Step: 724, Accuracy: 0.9966298342541436\n",
      "Step: 725, Accuracy: 0.9966344827586207\n",
      "Step: 726, Accuracy: 0.9966391184573002\n",
      "Step: 727, Accuracy: 0.9966437414030261\n",
      "Step: 728, Accuracy: 0.9966483516483516\n",
      "Step: 729, Accuracy: 0.9966529492455418\n",
      "Step: 730, Accuracy: 0.9966575342465753\n",
      "Step: 731, Accuracy: 0.9966621067031464\n",
      "Step: 732, Accuracy: 0.9966666666666667\n",
      "Step: 733, Accuracy: 0.9966712141882674\n",
      "Step: 734, Accuracy: 0.9966757493188011\n",
      "Step: 735, Accuracy: 0.9966802721088436\n",
      "Step: 736, Accuracy: 0.9966847826086956\n",
      "Step: 737, Accuracy: 0.9966892808683854\n",
      "Step: 738, Accuracy: 0.9966937669376694\n",
      "Step: 739, Accuracy: 0.9966982408660352\n",
      "Step: 740, Accuracy: 0.9967027027027027\n",
      "Step: 741, Accuracy: 0.9967071524966262\n",
      "Step: 742, Accuracy: 0.9967115902964959\n",
      "Step: 743, Accuracy: 0.9966890982503365\n",
      "Step: 744, Accuracy: 0.9966935483870968\n",
      "Step: 745, Accuracy: 0.9966979865771812\n",
      "Step: 746, Accuracy: 0.9967024128686327\n",
      "Step: 747, Accuracy: 0.996706827309237\n",
      "Step: 748, Accuracy: 0.9966577540106952\n",
      "Step: 749, Accuracy: 0.9966622162883845\n",
      "Step: 750, Accuracy: 0.99664\n",
      "Step: 751, Accuracy: 0.9966444740346205\n",
      "Step: 752, Accuracy: 0.9966489361702128\n",
      "Step: 753, Accuracy: 0.9966533864541832\n",
      "Step: 754, Accuracy: 0.996657824933687\n",
      "Step: 755, Accuracy: 0.9966357615894039\n",
      "Step: 756, Accuracy: 0.9966402116402117\n",
      "Step: 757, Accuracy: 0.9966446499339499\n",
      "Step: 758, Accuracy: 0.9966490765171504\n",
      "Step: 759, Accuracy: 0.996600790513834\n",
      "Step: 760, Accuracy: 0.9966052631578948\n",
      "Step: 761, Accuracy: 0.9966097240473062\n",
      "Step: 762, Accuracy: 0.9966141732283464\n",
      "Step: 763, Accuracy: 0.9966186107470512\n",
      "Step: 764, Accuracy: 0.9966230366492147\n",
      "Step: 765, Accuracy: 0.9966274509803922\n",
      "Step: 766, Accuracy: 0.9966318537859008\n",
      "Step: 767, Accuracy: 0.9966362451108214\n",
      "Step: 768, Accuracy: 0.996640625\n",
      "Step: 769, Accuracy: 0.9966449934980495\n",
      "Step: 770, Accuracy: 0.9966493506493507\n",
      "Step: 771, Accuracy: 0.9966536964980545\n",
      "Step: 772, Accuracy: 0.9966580310880829\n",
      "Step: 773, Accuracy: 0.9966623544631307\n",
      "Step: 774, Accuracy: 0.9966666666666667\n",
      "Step: 775, Accuracy: 0.9966709677419355\n",
      "Step: 776, Accuracy: 0.9966752577319588\n",
      "Step: 777, Accuracy: 0.9966795366795367\n",
      "Step: 778, Accuracy: 0.9966838046272494\n",
      "Step: 779, Accuracy: 0.9966623876765084\n",
      "Step: 780, Accuracy: 0.9966666666666667\n",
      "Step: 781, Accuracy: 0.9966453265044815\n",
      "Step: 782, Accuracy: 0.9966496163682864\n",
      "Step: 783, Accuracy: 0.9966538952745849\n",
      "Step: 784, Accuracy: 0.9966581632653061\n",
      "Step: 785, Accuracy: 0.9966624203821656\n",
      "Step: 786, Accuracy: 0.9966666666666667\n",
      "Step: 787, Accuracy: 0.9966200762388818\n",
      "Step: 788, Accuracy: 0.9966243654822335\n",
      "Step: 789, Accuracy: 0.9966286438529784\n",
      "Step: 790, Accuracy: 0.9966329113924051\n",
      "Step: 791, Accuracy: 0.9966371681415929\n",
      "Step: 792, Accuracy: 0.9966161616161616\n",
      "Step: 793, Accuracy: 0.9966204287515763\n",
      "Step: 794, Accuracy: 0.9966246851385391\n",
      "Step: 795, Accuracy: 0.9966289308176101\n",
      "Step: 796, Accuracy: 0.9966331658291457\n",
      "Step: 797, Accuracy: 0.9966373902132999\n",
      "Step: 798, Accuracy: 0.9966416040100251\n",
      "Step: 799, Accuracy: 0.9966207759699625\n",
      "Step: 800, Accuracy: 0.996625\n",
      "Step: 801, Accuracy: 0.9966042446941323\n",
      "Step: 802, Accuracy: 0.9966084788029925\n",
      "Step: 803, Accuracy: 0.996587795765878\n",
      "Step: 804, Accuracy: 0.996592039800995\n",
      "Step: 805, Accuracy: 0.9965962732919255\n",
      "Step: 806, Accuracy: 0.9966004962779156\n",
      "Step: 807, Accuracy: 0.9966047087980173\n",
      "Step: 808, Accuracy: 0.9966089108910892\n",
      "Step: 809, Accuracy: 0.9966131025957973\n",
      "Step: 810, Accuracy: 0.9966172839506173\n",
      "Step: 811, Accuracy: 0.9966214549938348\n",
      "Step: 812, Accuracy: 0.9966256157635468\n",
      "Step: 813, Accuracy: 0.996629766297663\n",
      "Step: 814, Accuracy: 0.9966339066339066\n",
      "Step: 815, Accuracy: 0.996638036809816\n",
      "Step: 816, Accuracy: 0.996642156862745\n",
      "Step: 817, Accuracy: 0.9966462668298653\n",
      "Step: 818, Accuracy: 0.9966259168704157\n",
      "Step: 819, Accuracy: 0.9966300366300366\n",
      "Step: 820, Accuracy: 0.9966341463414634\n",
      "Step: 821, Accuracy: 0.9966382460414129\n",
      "Step: 822, Accuracy: 0.9966423357664234\n",
      "Step: 823, Accuracy: 0.9966464155528554\n",
      "Step: 824, Accuracy: 0.9966504854368932\n",
      "Step: 825, Accuracy: 0.9966545454545455\n",
      "Step: 826, Accuracy: 0.9966585956416465\n",
      "Step: 827, Accuracy: 0.9966626360338573\n",
      "Step: 828, Accuracy: 0.9966666666666667\n",
      "Step: 829, Accuracy: 0.996670687575392\n",
      "Step: 830, Accuracy: 0.9966506024096385\n",
      "Step: 831, Accuracy: 0.9966546329723225\n",
      "Step: 832, Accuracy: 0.9966346153846154\n",
      "Step: 833, Accuracy: 0.9966386554621849\n",
      "Step: 834, Accuracy: 0.996642685851319\n",
      "Step: 835, Accuracy: 0.9966467065868263\n",
      "Step: 836, Accuracy: 0.9966507177033492\n",
      "Step: 837, Accuracy: 0.9966547192353644\n",
      "Step: 838, Accuracy: 0.9966587112171837\n",
      "Step: 839, Accuracy: 0.9966626936829559\n",
      "Step: 840, Accuracy: 0.9966666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 841, Accuracy: 0.9966706302021403\n",
      "Step: 842, Accuracy: 0.9966745843230403\n",
      "Step: 843, Accuracy: 0.9966548042704626\n",
      "Step: 844, Accuracy: 0.9966587677725118\n",
      "Step: 845, Accuracy: 0.9966390532544379\n",
      "Step: 846, Accuracy: 0.9966193853427896\n",
      "Step: 847, Accuracy: 0.9966233766233766\n",
      "Step: 848, Accuracy: 0.9966037735849057\n",
      "Step: 849, Accuracy: 0.99660777385159\n",
      "Step: 850, Accuracy: 0.9966117647058823\n",
      "Step: 851, Accuracy: 0.9966157461809636\n",
      "Step: 852, Accuracy: 0.9966197183098592\n",
      "Step: 853, Accuracy: 0.9966236811254396\n",
      "Step: 854, Accuracy: 0.9966276346604216\n",
      "Step: 855, Accuracy: 0.9966315789473684\n",
      "Step: 856, Accuracy: 0.9966121495327103\n",
      "Step: 857, Accuracy: 0.9966161026837806\n",
      "Step: 858, Accuracy: 0.9966200466200467\n",
      "Step: 859, Accuracy: 0.9966239813736903\n",
      "Step: 860, Accuracy: 0.9966279069767442\n",
      "Step: 861, Accuracy: 0.9966318234610918\n",
      "Step: 862, Accuracy: 0.9966357308584687\n",
      "Step: 863, Accuracy: 0.9966396292004635\n",
      "Step: 864, Accuracy: 0.9966435185185185\n",
      "Step: 865, Accuracy: 0.9966242774566474\n",
      "Step: 866, Accuracy: 0.9966281755196305\n",
      "Step: 867, Accuracy: 0.9966320645905421\n",
      "Step: 868, Accuracy: 0.9966359447004608\n",
      "Step: 869, Accuracy: 0.9966168009205983\n",
      "Step: 870, Accuracy: 0.9966206896551724\n",
      "Step: 871, Accuracy: 0.9966245694603904\n",
      "Step: 872, Accuracy: 0.9966284403669725\n",
      "Step: 873, Accuracy: 0.9966323024054983\n",
      "Step: 874, Accuracy: 0.9966361556064073\n",
      "Step: 875, Accuracy: 0.9966171428571429\n",
      "Step: 876, Accuracy: 0.9966210045662101\n",
      "Step: 877, Accuracy: 0.9966248574686432\n",
      "Step: 878, Accuracy: 0.996628701594533\n",
      "Step: 879, Accuracy: 0.9966097838452788\n",
      "Step: 880, Accuracy: 0.9966136363636363\n",
      "Step: 881, Accuracy: 0.996594778660613\n",
      "Step: 882, Accuracy: 0.9965759637188208\n",
      "Step: 883, Accuracy: 0.9965571913929785\n",
      "Step: 884, Accuracy: 0.9965610859728506\n",
      "Step: 885, Accuracy: 0.9965649717514125\n",
      "Step: 886, Accuracy: 0.996568848758465\n",
      "Step: 887, Accuracy: 0.9965727170236753\n",
      "Step: 888, Accuracy: 0.9965765765765766\n",
      "Step: 889, Accuracy: 0.9965804274465692\n",
      "Step: 890, Accuracy: 0.996561797752809\n",
      "Step: 891, Accuracy: 0.9965656565656565\n",
      "Step: 892, Accuracy: 0.9965695067264574\n",
      "Step: 893, Accuracy: 0.9965733482642777\n",
      "Step: 894, Accuracy: 0.9965771812080537\n",
      "Step: 895, Accuracy: 0.9965810055865921\n",
      "Step: 896, Accuracy: 0.9965625\n",
      "Step: 897, Accuracy: 0.9965440356744705\n",
      "Step: 898, Accuracy: 0.9965478841870824\n",
      "Step: 899, Accuracy: 0.996551724137931\n",
      "Step: 900, Accuracy: 0.9965555555555555\n",
      "Step: 901, Accuracy: 0.9965593784683685\n",
      "Step: 902, Accuracy: 0.9965631929046563\n",
      "Step: 903, Accuracy: 0.9965448504983389\n",
      "Step: 904, Accuracy: 0.9965486725663717\n",
      "Step: 905, Accuracy: 0.9965082872928177\n",
      "Step: 906, Accuracy: 0.9965121412803531\n",
      "Step: 907, Accuracy: 0.99651598676957\n",
      "Step: 908, Accuracy: 0.9965198237885463\n",
      "Step: 909, Accuracy: 0.9965236523652365\n",
      "Step: 910, Accuracy: 0.9965274725274725\n",
      "Step: 911, Accuracy: 0.9965312843029638\n",
      "Step: 912, Accuracy: 0.9965350877192982\n",
      "Step: 913, Accuracy: 0.9965388828039431\n",
      "Step: 914, Accuracy: 0.9965426695842451\n",
      "Step: 915, Accuracy: 0.9965464480874316\n",
      "Step: 916, Accuracy: 0.9965502183406113\n",
      "Step: 917, Accuracy: 0.9965539803707743\n",
      "Step: 918, Accuracy: 0.996557734204793\n",
      "Step: 919, Accuracy: 0.9965614798694233\n",
      "Step: 920, Accuracy: 0.9965652173913043\n",
      "Step: 921, Accuracy: 0.9965472312703583\n",
      "Step: 922, Accuracy: 0.9965509761388286\n",
      "Step: 923, Accuracy: 0.9965330444203684\n",
      "Step: 924, Accuracy: 0.9965151515151515\n",
      "Step: 925, Accuracy: 0.996518918918919\n",
      "Step: 926, Accuracy: 0.9965226781857451\n",
      "Step: 927, Accuracy: 0.9965264293419633\n",
      "Step: 928, Accuracy: 0.9965301724137932\n",
      "Step: 929, Accuracy: 0.9965339074273413\n",
      "Step: 930, Accuracy: 0.9965376344086021\n",
      "Step: 931, Accuracy: 0.9965413533834586\n",
      "Step: 932, Accuracy: 0.9965450643776824\n",
      "Step: 933, Accuracy: 0.9965487674169347\n",
      "Step: 934, Accuracy: 0.9965524625267665\n",
      "Step: 935, Accuracy: 0.9965347593582887\n",
      "Step: 936, Accuracy: 0.9965384615384615\n",
      "Step: 937, Accuracy: 0.9965421558164355\n",
      "Step: 938, Accuracy: 0.996545842217484\n",
      "Step: 939, Accuracy: 0.9965495207667732\n",
      "Step: 940, Accuracy: 0.9965531914893617\n",
      "Step: 941, Accuracy: 0.9965568544102019\n",
      "Step: 942, Accuracy: 0.9965605095541401\n",
      "Step: 943, Accuracy: 0.9965641569459173\n",
      "Step: 944, Accuracy: 0.9965677966101695\n",
      "Step: 945, Accuracy: 0.9965502645502645\n",
      "Step: 946, Accuracy: 0.996553911205074\n",
      "Step: 947, Accuracy: 0.996557550158395\n",
      "Step: 948, Accuracy: 0.9965611814345992\n",
      "Step: 949, Accuracy: 0.9965648050579557\n",
      "Step: 950, Accuracy: 0.9965684210526315\n",
      "Step: 951, Accuracy: 0.9965720294426919\n",
      "Step: 952, Accuracy: 0.9965546218487394\n",
      "Step: 953, Accuracy: 0.9965582371458552\n",
      "Step: 954, Accuracy: 0.9965618448637317\n",
      "Step: 955, Accuracy: 0.996565445026178\n",
      "Step: 956, Accuracy: 0.9965690376569037\n",
      "Step: 957, Accuracy: 0.9965726227795193\n",
      "Step: 958, Accuracy: 0.9965762004175366\n",
      "Step: 959, Accuracy: 0.9965797705943691\n",
      "Step: 960, Accuracy: 0.9965833333333334\n",
      "Step: 961, Accuracy: 0.9965660770031217\n",
      "Step: 962, Accuracy: 0.9965696465696465\n",
      "Step: 963, Accuracy: 0.9965732087227415\n",
      "Step: 964, Accuracy: 0.9965767634854772\n",
      "Step: 965, Accuracy: 0.996580310880829\n",
      "Step: 966, Accuracy: 0.996583850931677\n",
      "Step: 967, Accuracy: 0.9965873836608066\n",
      "Step: 968, Accuracy: 0.9965909090909091\n",
      "Step: 969, Accuracy: 0.9965737874097007\n",
      "Step: 970, Accuracy: 0.9965773195876289\n",
      "Step: 971, Accuracy: 0.9965808444902162\n",
      "Step: 972, Accuracy: 0.9965637860082305\n",
      "Step: 973, Accuracy: 0.9965673175745118\n",
      "Step: 974, Accuracy: 0.9965503080082135\n",
      "Step: 975, Accuracy: 0.9965538461538461\n",
      "Step: 976, Accuracy: 0.9965573770491803\n",
      "Step: 977, Accuracy: 0.9965609007164791\n",
      "Step: 978, Accuracy: 0.9965235173824131\n",
      "Step: 979, Accuracy: 0.9965270684371808\n",
      "Step: 980, Accuracy: 0.9965306122448979\n",
      "Step: 981, Accuracy: 0.9965341488277268\n",
      "Step: 982, Accuracy: 0.9965376782077393\n",
      "Step: 983, Accuracy: 0.9965412004069176\n",
      "Step: 984, Accuracy: 0.9965447154471545\n",
      "Step: 985, Accuracy: 0.9965279187817259\n",
      "Step: 986, Accuracy: 0.9965111561866126\n",
      "Step: 987, Accuracy: 0.9965146909827761\n",
      "Step: 988, Accuracy: 0.9965182186234818\n",
      "Step: 989, Accuracy: 0.9965015166835187\n",
      "Step: 990, Accuracy: 0.9965050505050506\n",
      "Step: 991, Accuracy: 0.9965085771947527\n",
      "Step: 992, Accuracy: 0.9965120967741935\n",
      "Step: 993, Accuracy: 0.9965156092648539\n",
      "Step: 994, Accuracy: 0.9965191146881288\n",
      "Step: 995, Accuracy: 0.9965226130653266\n",
      "Step: 996, Accuracy: 0.9965261044176706\n",
      "Step: 997, Accuracy: 0.9965295887662989\n",
      "Step: 998, Accuracy: 0.9965330661322646\n",
      "Step: 999, Accuracy: 0.9965365365365365\n",
      "Step: 1000, Accuracy: 0.99654\n",
      "Step: 1001, Accuracy: 0.9965434565434566\n",
      "Step: 1002, Accuracy: 0.9965269461077845\n",
      "Step: 1003, Accuracy: 0.9965304087736789\n",
      "Step: 1004, Accuracy: 0.9965338645418327\n",
      "Step: 1005, Accuracy: 0.9965174129353234\n",
      "Step: 1006, Accuracy: 0.9965208747514911\n",
      "Step: 1007, Accuracy: 0.9965243296921549\n",
      "Step: 1008, Accuracy: 0.9965277777777778\n",
      "Step: 1009, Accuracy: 0.9965113974231913\n",
      "Step: 1010, Accuracy: 0.9965148514851485\n",
      "Step: 1011, Accuracy: 0.9965182987141444\n",
      "Step: 1012, Accuracy: 0.9965217391304347\n",
      "Step: 1013, Accuracy: 0.9965251727541955\n",
      "Step: 1014, Accuracy: 0.9965285996055226\n",
      "Step: 1015, Accuracy: 0.9965320197044335\n",
      "Step: 1016, Accuracy: 0.9965354330708661\n",
      "Step: 1017, Accuracy: 0.9965388397246804\n",
      "Step: 1018, Accuracy: 0.9965422396856581\n",
      "Step: 1019, Accuracy: 0.9965260058881256\n",
      "Step: 1020, Accuracy: 0.9965098039215686\n",
      "Step: 1021, Accuracy: 0.996513222331048\n",
      "Step: 1022, Accuracy: 0.9965166340508806\n",
      "Step: 1023, Accuracy: 0.9965200391006842\n",
      "Step: 1024, Accuracy: 0.99650390625\n",
      "Step: 1025, Accuracy: 0.9964878048780488\n",
      "Step: 1026, Accuracy: 0.9964912280701754\n",
      "Step: 1027, Accuracy: 0.9964946445959104\n",
      "Step: 1028, Accuracy: 0.9964980544747082\n",
      "Step: 1029, Accuracy: 0.9965014577259476\n",
      "Step: 1030, Accuracy: 0.9964854368932039\n",
      "Step: 1031, Accuracy: 0.9964888457807953\n",
      "Step: 1032, Accuracy: 0.9964922480620155\n",
      "Step: 1033, Accuracy: 0.9964956437560504\n",
      "Step: 1034, Accuracy: 0.9964796905222437\n",
      "Step: 1035, Accuracy: 0.9964830917874397\n",
      "Step: 1036, Accuracy: 0.9964864864864865\n",
      "Step: 1037, Accuracy: 0.9964898746383799\n",
      "Step: 1038, Accuracy: 0.9964739884393063\n",
      "Step: 1039, Accuracy: 0.9964773820981713\n",
      "Step: 1040, Accuracy: 0.9964807692307692\n",
      "Step: 1041, Accuracy: 0.9964841498559078\n",
      "Step: 1042, Accuracy: 0.9964875239923224\n",
      "Step: 1043, Accuracy: 0.9964908916586769\n",
      "Step: 1044, Accuracy: 0.9964942528735632\n",
      "Step: 1045, Accuracy: 0.9964976076555024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1046, Accuracy: 0.9965009560229445\n",
      "Step: 1047, Accuracy: 0.9964851957975167\n",
      "Step: 1048, Accuracy: 0.996469465648855\n",
      "Step: 1049, Accuracy: 0.9964728312678741\n",
      "Step: 1050, Accuracy: 0.9964761904761905\n",
      "Step: 1051, Accuracy: 0.9964795432921028\n",
      "Step: 1052, Accuracy: 0.9964828897338402\n",
      "Step: 1053, Accuracy: 0.9964862298195631\n",
      "Step: 1054, Accuracy: 0.9964895635673624\n",
      "Step: 1055, Accuracy: 0.9964928909952606\n",
      "Step: 1056, Accuracy: 0.9964962121212121\n",
      "Step: 1057, Accuracy: 0.996480605487228\n",
      "Step: 1058, Accuracy: 0.9964650283553875\n",
      "Step: 1059, Accuracy: 0.9964494806421152\n",
      "Step: 1060, Accuracy: 0.9964528301886793\n",
      "Step: 1061, Accuracy: 0.9964373232799246\n",
      "Step: 1062, Accuracy: 0.9964406779661017\n",
      "Step: 1063, Accuracy: 0.9964440263405456\n",
      "Step: 1064, Accuracy: 0.9964473684210526\n",
      "Step: 1065, Accuracy: 0.9964507042253521\n",
      "Step: 1066, Accuracy: 0.996454033771107\n",
      "Step: 1067, Accuracy: 0.9964573570759138\n",
      "Step: 1068, Accuracy: 0.996441947565543\n",
      "Step: 1069, Accuracy: 0.99644527595884\n",
      "Step: 1070, Accuracy: 0.9964485981308411\n",
      "Step: 1071, Accuracy: 0.9964519140989729\n",
      "Step: 1072, Accuracy: 0.996455223880597\n",
      "Step: 1073, Accuracy: 0.9964585274930102\n",
      "Step: 1074, Accuracy: 0.996461824953445\n",
      "Step: 1075, Accuracy: 0.9964651162790698\n",
      "Step: 1076, Accuracy: 0.9964684014869889\n",
      "Step: 1077, Accuracy: 0.9964716805942433\n",
      "Step: 1078, Accuracy: 0.9964749536178108\n",
      "Step: 1079, Accuracy: 0.9964782205746061\n",
      "Step: 1080, Accuracy: 0.9964814814814815\n",
      "Step: 1081, Accuracy: 0.9964847363552266\n",
      "Step: 1082, Accuracy: 0.9964879852125693\n",
      "Step: 1083, Accuracy: 0.9964912280701754\n",
      "Step: 1084, Accuracy: 0.9964944649446494\n",
      "Step: 1085, Accuracy: 0.9964976958525346\n",
      "Step: 1086, Accuracy: 0.996500920810313\n",
      "Step: 1087, Accuracy: 0.9965041398344067\n",
      "Step: 1088, Accuracy: 0.9964889705882353\n",
      "Step: 1089, Accuracy: 0.9964921946740128\n",
      "Step: 1090, Accuracy: 0.9964770642201835\n",
      "Step: 1091, Accuracy: 0.9964802933088909\n",
      "Step: 1092, Accuracy: 0.9964835164835165\n",
      "Step: 1093, Accuracy: 0.9964867337602927\n",
      "Step: 1094, Accuracy: 0.996489945155393\n",
      "Step: 1095, Accuracy: 0.9964931506849315\n",
      "Step: 1096, Accuracy: 0.9964963503649635\n",
      "Step: 1097, Accuracy: 0.9964995442114859\n",
      "Step: 1098, Accuracy: 0.9965027322404372\n",
      "Step: 1099, Accuracy: 0.9965059144676979\n",
      "Step: 1100, Accuracy: 0.9965090909090909\n",
      "Step: 1101, Accuracy: 0.9965122615803814\n",
      "Step: 1102, Accuracy: 0.9965154264972776\n",
      "Step: 1103, Accuracy: 0.9965185856754306\n",
      "Step: 1104, Accuracy: 0.9965217391304347\n",
      "Step: 1105, Accuracy: 0.996524886877828\n",
      "Step: 1106, Accuracy: 0.9965280289330922\n",
      "Step: 1107, Accuracy: 0.9965311653116531\n",
      "Step: 1108, Accuracy: 0.9965342960288809\n",
      "Step: 1109, Accuracy: 0.9965374211000901\n",
      "Step: 1110, Accuracy: 0.9965405405405405\n",
      "Step: 1111, Accuracy: 0.9965436543654366\n",
      "Step: 1112, Accuracy: 0.996546762589928\n",
      "Step: 1113, Accuracy: 0.9965498652291105\n",
      "Step: 1114, Accuracy: 0.9965529622980251\n",
      "Step: 1115, Accuracy: 0.9965560538116592\n",
      "Step: 1116, Accuracy: 0.9965591397849463\n",
      "Step: 1117, Accuracy: 0.9965622202327663\n",
      "Step: 1118, Accuracy: 0.9965652951699463\n",
      "Step: 1119, Accuracy: 0.9965683646112601\n",
      "Step: 1120, Accuracy: 0.9965714285714286\n",
      "Step: 1121, Accuracy: 0.9965744870651204\n",
      "Step: 1122, Accuracy: 0.9965775401069519\n",
      "Step: 1123, Accuracy: 0.996580587711487\n",
      "Step: 1124, Accuracy: 0.9965836298932385\n",
      "Step: 1125, Accuracy: 0.9965688888888888\n",
      "Step: 1126, Accuracy: 0.9965719360568384\n",
      "Step: 1127, Accuracy: 0.9965749778172138\n",
      "Step: 1128, Accuracy: 0.9965780141843972\n",
      "Step: 1129, Accuracy: 0.9965810451727192\n",
      "Step: 1130, Accuracy: 0.9965840707964602\n",
      "Step: 1131, Accuracy: 0.9965870910698497\n",
      "Step: 1132, Accuracy: 0.9965901060070671\n",
      "Step: 1133, Accuracy: 0.9965931156222418\n",
      "Step: 1134, Accuracy: 0.9965961199294533\n",
      "Step: 1135, Accuracy: 0.9965814977973568\n",
      "Step: 1136, Accuracy: 0.9965845070422535\n",
      "Step: 1137, Accuracy: 0.9965875109938435\n",
      "Step: 1138, Accuracy: 0.9965905096660809\n",
      "Step: 1139, Accuracy: 0.996593503072871\n",
      "Step: 1140, Accuracy: 0.9965964912280701\n",
      "Step: 1141, Accuracy: 0.9965994741454864\n",
      "Step: 1142, Accuracy: 0.9966024518388792\n",
      "Step: 1143, Accuracy: 0.9966054243219598\n",
      "Step: 1144, Accuracy: 0.9966083916083917\n",
      "Step: 1145, Accuracy: 0.996593886462882\n",
      "Step: 1146, Accuracy: 0.9965968586387435\n",
      "Step: 1147, Accuracy: 0.9965998256320837\n",
      "Step: 1148, Accuracy: 0.996602787456446\n",
      "Step: 1149, Accuracy: 0.9966057441253263\n",
      "Step: 1150, Accuracy: 0.9966086956521739\n",
      "Step: 1151, Accuracy: 0.996611642050391\n",
      "Step: 1152, Accuracy: 0.9965972222222222\n",
      "Step: 1153, Accuracy: 0.996582827406765\n",
      "Step: 1154, Accuracy: 0.9965857885615251\n",
      "Step: 1155, Accuracy: 0.9965887445887446\n",
      "Step: 1156, Accuracy: 0.9965743944636678\n",
      "Step: 1157, Accuracy: 0.9965773552290407\n",
      "Step: 1158, Accuracy: 0.996580310880829\n",
      "Step: 1159, Accuracy: 0.9965832614322692\n",
      "Step: 1160, Accuracy: 0.9965862068965518\n",
      "Step: 1161, Accuracy: 0.9965891472868217\n",
      "Step: 1162, Accuracy: 0.996592082616179\n",
      "Step: 1163, Accuracy: 0.9965950128976784\n",
      "Step: 1164, Accuracy: 0.9965979381443298\n",
      "Step: 1165, Accuracy: 0.9966008583690987\n",
      "Step: 1166, Accuracy: 0.9966037735849057\n",
      "Step: 1167, Accuracy: 0.9965895458440446\n",
      "Step: 1168, Accuracy: 0.9965924657534246\n",
      "Step: 1169, Accuracy: 0.996595380667237\n",
      "Step: 1170, Accuracy: 0.9965982905982906\n",
      "Step: 1171, Accuracy: 0.996601195559351\n",
      "Step: 1172, Accuracy: 0.9966040955631399\n",
      "Step: 1173, Accuracy: 0.9966069906223359\n",
      "Step: 1174, Accuracy: 0.9966098807495741\n",
      "Step: 1175, Accuracy: 0.9966127659574469\n",
      "Step: 1176, Accuracy: 0.9966156462585034\n",
      "Step: 1177, Accuracy: 0.9966185216652507\n",
      "Step: 1178, Accuracy: 0.9966213921901528\n",
      "Step: 1179, Accuracy: 0.9966242578456319\n",
      "Step: 1180, Accuracy: 0.9966271186440678\n",
      "Step: 1181, Accuracy: 0.9966299745977985\n",
      "Step: 1182, Accuracy: 0.9966328257191202\n",
      "Step: 1183, Accuracy: 0.9966187658495351\n",
      "Step: 1184, Accuracy: 0.9966216216216216\n",
      "Step: 1185, Accuracy: 0.9966244725738397\n",
      "Step: 1186, Accuracy: 0.9966273187183811\n",
      "Step: 1187, Accuracy: 0.9966301600673968\n",
      "Step: 1188, Accuracy: 0.9966329966329966\n",
      "Step: 1189, Accuracy: 0.9966358284272497\n",
      "Step: 1190, Accuracy: 0.9966386554621849\n",
      "Step: 1191, Accuracy: 0.9966414777497901\n",
      "Step: 1192, Accuracy: 0.9966442953020134\n",
      "Step: 1193, Accuracy: 0.9966303436714166\n",
      "Step: 1194, Accuracy: 0.9966331658291457\n",
      "Step: 1195, Accuracy: 0.9966359832635984\n",
      "Step: 1196, Accuracy: 0.9966387959866221\n",
      "Step: 1197, Accuracy: 0.996624895572264\n",
      "Step: 1198, Accuracy: 0.9966277128547579\n",
      "Step: 1199, Accuracy: 0.9966305254378649\n",
      "Step: 1200, Accuracy: 0.9966333333333334\n",
      "Final Accuracy: 0.9966333333333334\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "diffy.eval()\n",
    "for i in range(testsize):\n",
    "    with T.no_grad():\n",
    "        img, tru_out = next(testset)\n",
    "    \n",
    "        out, (c, m, r) = diffy(img.reshape((50, 28, 28)), (None, memory, None), reset_experience=True)\n",
    "        m = { k : (v.detach() if isinstance(v, T.autograd.Variable) else v) for k, v in m.items()}\n",
    "        (c, m, r) = (None, None, None)\n",
    "        y_pred = T.sum(out, (1, 2)).round()\n",
    "        \n",
    "        for b in range(50):\n",
    "            if(y_pred[b] == tru_out[b]):\n",
    "                correct = correct +1\n",
    "        \n",
    "        print('Step: {}, Accuracy: {}'.format(i+1, correct/((i+1)*50)))\n",
    "print('Final Accuracy: {}'.format(correct/60000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('DNC-model-2', 'wb')\n",
    "pickle.dump(diffy, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('DNC-memory-2', 'wb')\n",
    "pickle.dump(memory, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.reshape((50, 28, 28)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
